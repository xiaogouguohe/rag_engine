# 召回率评估详解

## 一、生成的问题示例

### 基于 HowToCook 知识库

假设知识库中有以下文档：

```
dishes/
├── meat_dish/
│   ├── 西红柿鸡蛋.md
│   ├── 红烧肉.md
│   └── 宫保鸡丁.md
├── vegetable_dish/
│   ├── 麻婆豆腐.md
│   ├── 地三鲜.md
│   └── 西红柿豆腐汤羹.md
└── soup/
    └── 西红柿鸡蛋汤.md
```

### 生成的问题（策略1：从文件名生成）

**主要问题类型**：`如何做{菜名}？`

| 文件名 | 生成的问题 |
|--------|-----------|
| `西红柿鸡蛋.md` | `如何做西红柿鸡蛋？` |
| `红烧肉.md` | `如何做红烧肉？` |
| `宫保鸡丁.md` | `如何做宫保鸡丁？` |
| `麻婆豆腐.md` | `如何做麻婆豆腐？` |
| `地三鲜.md` | `如何做地三鲜？` |
| `西红柿豆腐汤羹.md` | `如何做西红柿豆腐汤羹？` |
| `西红柿鸡蛋汤.md` | `如何做西红柿鸡蛋汤？` |

**特点**：
- ✅ 问题格式统一（"如何做..."）
- ✅ 问题与文档高度相关
- ✅ 快速生成，无需调用 LLM

### 生成的问题（策略2：使用 LLM 生成，可选）

如果启用 LLM 生成，对于 `西红柿鸡蛋.md`，可能生成：

1. `如何做西红柿鸡蛋？`（文件名生成）
2. `西红柿鸡蛋需要哪些原料？`（LLM 生成）
3. `西红柿鸡蛋的难度是多少？`（LLM 生成）
4. `西红柿鸡蛋的制作步骤是什么？`（LLM 生成）

**特点**：
- ✅ 问题多样化
- ✅ 更贴近真实用户问题
- ⚠️ 需要调用 LLM，成本较高

## 二、召回率评估逻辑

### 核心思路

**如果问题来自文档 A，那么文档 A 的所有块都是相关的。**

### 评估流程

```
问题："如何做西红柿鸡蛋？"
   ↓
相关文档：parent_id = "a1b2c3d4e5f6..."（整个文档的所有块）
   ↓
向量检索（top_k=5）：
  检索到的文档块及其 parent_id
   ↓
判断是否检索到相关文档：
  如果检索结果中包含 parent_id = "a1b2c3d4e5f6..."
  则召回率 = 1.0（检索到了）
  否则召回率 = 0.0（未检索到）
```

### 代码实现

```python
# 1. 获取相关文档的 parent_id
relevant_parent_ids = set(sample.get("relevant_chunks", []))
# 例如：{"a1b2c3d4e5f6..."}

# 2. 进行向量检索
query_vector = embed(question)
search_results = vector_store.search(query_vector, top_k=5)

# 3. 获取检索到的 parent_id
retrieved_parent_ids = set()
for score, metadata in search_results:
    parent_id = metadata.metadata.get("parent_id")
    if parent_id:
        retrieved_parent_ids.add(parent_id)

# 4. 计算召回率
relevant_retrieved = relevant_parent_ids.intersection(retrieved_parent_ids)

# 简化计算：因为整个文档都是相关的
# 如果检索到相关文档，召回率 = 1.0
# 如果未检索到，召回率 = 0.0
if relevant_retrieved:
    recall = 1.0  # 检索到了相关文档
else:
    recall = 0.0  # 未检索到相关文档
```

## 三、召回率计算示例

### 示例1：完美检索（召回率 = 1.0）

```
问题："如何做西红柿鸡蛋？"
相关文档：parent_id = "a1b2c3d4e5f6..."

检索结果（top_k=5）：
  1. parent_id = "a1b2c3d4e5f6..." (相关) ✅
  2. parent_id = "b2c3d4e5f6a1..." (不相关)
  3. parent_id = "a1b2c3d4e5f6..." (相关) ✅
  4. parent_id = "c3d4e5f6a1b2..." (不相关)
  5. parent_id = "d4e5f6a1b2c3..." (不相关)

结果：检索到了相关文档
召回率 = 1.0 (100%)
```

### 示例2：未检索到（召回率 = 0.0）

```
问题："如何做西红柿鸡蛋？"
相关文档：parent_id = "a1b2c3d4e5f6..."

检索结果（top_k=5）：
  1. parent_id = "b2c3d4e5f6a1..." (不相关)
  2. parent_id = "c3d4e5f6a1b2..." (不相关)
  3. parent_id = "d4e5f6a1b2c3..." (不相关)
  4. parent_id = "e5f6a1b2c3d4..." (不相关)
  5. parent_id = "f6a1b2c3d4e5..." (不相关)

结果：未检索到相关文档
召回率 = 0.0 (0%)
```

### 示例3：部分检索（召回率 = 1.0，因为整个文档都是相关的）

```
问题："如何做西红柿鸡蛋？"
相关文档：parent_id = "a1b2c3d4e5f6..."

检索结果（top_k=5）：
  1. parent_id = "a1b2c3d4e5f6..." (相关) ✅
  2. parent_id = "b2c3d4e5f6a1..." (不相关)
  3. parent_id = "c3d4e5f6a1b2..." (不相关)
  4. parent_id = "d4e5f6a1b2c3..." (不相关)
  5. parent_id = "e5f6a1b2c3d4..." (不相关)

结果：检索到了相关文档（即使只检索到 1 个块）
召回率 = 1.0 (100%)
```

**注意**：因为我们将整个文档的所有块都标记为相关，所以只要检索到该文档的任何一个块，召回率就是 1.0。

## 四、实际评估输出示例

### 评估过程输出

```
============================================================
RAG 检索阶段评估
============================================================
加载评估数据集...
✅ 加载 150 个评估样本
------------------------------------------------------------
初始化 RAG 引擎...
✅ RAG 引擎初始化成功
------------------------------------------------------------

开始评估（top_k=5）...

[1/150] 如何做西红柿鸡蛋？... Recall=1.00, Precision=0.20, MRR=1.00
[2/150] 如何做红烧肉？... Recall=1.00, Precision=0.20, MRR=1.00
[3/150] 如何做宫保鸡丁？... Recall=0.00, Precision=0.00, MRR=0.00
[4/150] 如何做麻婆豆腐？... Recall=1.00, Precision=0.20, MRR=1.00
[5/150] 如何做地三鲜？... Recall=1.00, Precision=0.20, MRR=0.50
...

============================================================
评估结果
============================================================
评估样本数: 150
Top-K: 5

指标:
  Recall@5:     0.8500 (85.00%)
  Precision@5:  0.1700 (17.00%)
  MRR:          0.7500
  Hit Rate@5:   0.8500 (85.00%)
  F1 Score:     0.2833
```

### 结果解读

- **Recall@5 = 85%**：150 个问题中，有 85% 的问题检索到了相关文档
- **Hit Rate@5 = 85%**：150 个问题中，有 85% 的问题至少检索到 1 个相关文档块
- **MRR = 0.75**：平均倒数排名为 0.75，说明相关文档的平均排名约为第 1.33 位

## 五、召回率评估的关键点

### 1. 简化假设

**当前实现**：整个文档的所有块都是相关的

**影响**：
- ✅ 简化了标注工作
- ✅ 只要检索到文档的任何一个块，召回率就是 1.0
- ⚠️ 可能过于宽松（某些块可能不相关）

### 2. 评估粒度

**文档级别**（当前实现）：
- 评估是否检索到相关文档
- 召回率 = 1.0（检索到）或 0.0（未检索到）

**块级别**（更精确，需要扩展）：
- 评估是否检索到相关块
- 召回率 = 检索到的相关块数 / 所有相关块数
- 可以计算 0.0 到 1.0 之间的值

### 3. 召回率的意义

**高召回率（> 0.8）**：
- ✅ 系统能找到大部分相关信息
- ✅ 不容易遗漏关键信息
- ✅ 适合需要全面信息的场景

**低召回率（< 0.6）**：
- ❌ 系统可能遗漏重要信息
- ❌ 需要优化检索策略
- ❌ 可能需要增加 top_k 或优化 embedding 模型

## 六、改进方向

### 1. 更精确的块级别评估

```python
# 标注每个块的相关性
relevant_chunks = ["chunk_1", "chunk_2"]  # 只有这些块是相关的
irrelevant_chunks = ["chunk_3", "chunk_4"]  # 其他块不相关

# 计算精确的召回率
retrieved_chunks = ["chunk_1", "chunk_3", "chunk_5"]
relevant_retrieved = set(retrieved_chunks) & set(relevant_chunks)
recall = len(relevant_retrieved) / len(relevant_chunks)  # 例如：1/2 = 0.5
```

### 2. 使用 LLM 判断块的相关性

```python
# 对每个块，使用 LLM 判断是否与问题相关
for chunk in document_chunks:
    is_relevant = llm_judge_relevance(question, chunk.text)
    if is_relevant:
        relevant_chunks.append(chunk.id)
```

## 七、总结

### 生成的问题类型

1. **主要类型**：`如何做{菜名}？`（从文件名生成）
   - 示例：`如何做西红柿鸡蛋？`、`如何做红烧肉？`
2. **次要类型**：多样化问题（使用 LLM 生成，可选）
   - 示例：`西红柿鸡蛋需要哪些原料？`、`西红柿鸡蛋的难度是多少？`

### 召回率评估

- **当前实现**：文档级别评估
  - 如果检索到相关文档，召回率 = 1.0
  - 如果未检索到，召回率 = 0.0
- **评估指标**：Recall@k、Hit Rate@k、MRR
- **改进方向**：块级别评估，更精确的召回率计算

