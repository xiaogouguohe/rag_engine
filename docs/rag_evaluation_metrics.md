# RAG 系统评估标准（面试要点）

## 一、评估体系概述

RAG 系统的评估分为三个层次：
1. **检索阶段（Retrieval）**：评估检索到的文档块是否相关
2. **生成阶段（Generation）**：评估生成的回答质量
3. **端到端（End-to-End）**：评估整个系统的综合表现

## 二、检索阶段评估指标

### 1. 召回率（Recall）⭐ **最重要**

**定义**：检索到的相关文档块数量 / 所有相关文档块数量

**公式**：
```
Recall = |检索到的相关文档块| / |所有相关文档块|
```

**意义**：
- 衡量系统是否找到了所有相关信息
- **高召回率**：说明系统不容易遗漏相关信息
- **低召回率**：说明系统可能遗漏重要信息

**示例**：
```
问题："如何做西红柿鸡蛋？"
所有相关文档块：10 个（包含西红柿鸡蛋做法的文档块）
检索到的相关文档块：7 个
召回率 = 7/10 = 0.7 (70%)
```

**面试要点**：
- 召回率是 RAG 系统最重要的指标之一
- 如果召回率低，即使生成质量再好，也可能因为缺少关键信息而回答错误
- 提高召回率的方法：增加 top_k、优化 embedding 模型、使用混合检索等

### 2. 精确率（Precision）

**定义**：检索到的相关文档块数量 / 检索到的所有文档块数量

**公式**：
```
Precision = |检索到的相关文档块| / |检索到的所有文档块|
```

**意义**：
- 衡量检索结果的准确性
- **高精确率**：检索到的文档块大部分都是相关的
- **低精确率**：检索结果中包含很多不相关的文档块

**示例**：
```
检索到的文档块：10 个
其中相关的文档块：7 个
精确率 = 7/10 = 0.7 (70%)
```

**召回率 vs 精确率**：
- **高召回率 + 低精确率**：找到了所有相关信息，但包含很多噪音
- **低召回率 + 高精确率**：检索结果很准确，但可能遗漏重要信息
- **理想状态**：高召回率 + 高精确率

### 3. F1 分数（F1-Score）

**定义**：召回率和精确率的调和平均数

**公式**：
```
F1 = 2 * (Precision * Recall) / (Precision + Recall)
```

**意义**：
- 综合衡量召回率和精确率
- 当召回率和精确率都很重要时，使用 F1 分数

### 4. MRR（Mean Reciprocal Rank）

**定义**：平均倒数排名，衡量第一个相关文档块的平均排名

**公式**：
```
MRR = (1/N) * Σ(1/rank_i)
其中 rank_i 是第 i 个查询中第一个相关文档块的排名
```

**意义**：
- 衡量系统是否能快速找到相关文档
- **MRR 越高**：相关文档越靠前
- **MRR = 1.0**：所有查询的第一个结果都是相关的

**示例**：
```
查询1：第一个相关文档块在第 2 位 → 1/2 = 0.5
查询2：第一个相关文档块在第 1 位 → 1/1 = 1.0
查询3：第一个相关文档块在第 3 位 → 1/3 = 0.33
MRR = (0.5 + 1.0 + 0.33) / 3 = 0.61
```

### 5. NDCG（Normalized Discounted Cumulative Gain）

**定义**：归一化折损累积增益，考虑文档块的相关性程度和排名位置

**公式**：
```
NDCG@k = DCG@k / IDCG@k
其中：
- DCG@k = Σ(relevance_i / log2(i+1))  # i 从 1 到 k
- IDCG@k = 理想情况下的 DCG（按相关性排序）
```

**意义**：
- 不仅考虑文档是否相关，还考虑相关性的程度（如：高度相关、中度相关、不相关）
- 排名越靠前，权重越大
- **NDCG 越高**：检索结果质量越好

**示例**：
```
检索结果（top-5）：
1. 高度相关（relevance=3）→ 3 / log2(2) = 3.0
2. 中度相关（relevance=2）→ 2 / log2(3) = 1.26
3. 不相关（relevance=0）→ 0 / log2(4) = 0
4. 高度相关（relevance=3）→ 3 / log2(5) = 1.29
5. 中度相关（relevance=2）→ 2 / log2(6) = 0.77

DCG@5 = 3.0 + 1.26 + 0 + 1.29 + 0.77 = 6.32
理想排序：3, 3, 2, 2, 0 → IDCG@5 = 7.14
NDCG@5 = 6.32 / 7.14 = 0.89
```

### 6. Hit Rate（命中率）

**定义**：至少检索到一个相关文档块的查询比例

**公式**：
```
Hit Rate@k = |至少有一个相关文档块在 top-k 中的查询| / |总查询数|
```

**意义**：
- 衡量系统是否能找到至少一个相关文档
- **Hit Rate 越高**：系统越不容易完全找不到相关信息

**示例**：
```
100 个查询
其中 85 个查询在 top-5 中至少有一个相关文档块
Hit Rate@5 = 85/100 = 0.85 (85%)
```

## 三、生成阶段评估指标

### 1. 忠实度（Faithfulness）

**定义**：生成的回答是否忠实于检索到的文档内容

**评估方式**：
- **人工评估**：人工判断回答是否基于检索到的文档
- **自动评估**：使用 LLM 判断回答是否可以从文档中推断出来

**意义**：
- 防止模型"幻觉"（hallucination）
- 确保回答有据可依

**示例**：
```
检索到的文档："西红柿鸡蛋需要西红柿、鸡蛋、盐"
生成的回答："西红柿鸡蛋需要西红柿、鸡蛋、盐、糖"  # 不忠实（添加了糖）
生成的回答："西红柿鸡蛋需要西红柿、鸡蛋、盐"      # 忠实
```

### 2. 相关性（Relevance）

**定义**：生成的回答是否与问题相关

**评估方式**：
- **人工评估**：人工判断回答是否回答了问题
- **自动评估**：使用 LLM 判断回答的相关性

**意义**：
- 确保回答不跑题
- 衡量回答是否解决了用户的问题

### 3. BLEU 分数

**定义**：基于 n-gram 匹配的文本相似度

**公式**：
```
BLEU = BP * exp(Σ(log(P_n)))
其中：
- P_n = n-gram 精确率
- BP = 简短惩罚（brevity penalty）
```

**意义**：
- 衡量生成文本与参考文本的相似度
- 主要用于机器翻译任务
- **BLEU 越高**：生成文本越接近参考文本

**局限性**：
- 只考虑表面相似度，不考虑语义
- 不适合评估 RAG 系统（因为可能有多种正确的回答方式）

### 4. ROUGE 分数

**定义**：基于召回率的文本相似度（主要用于摘要任务）

**类型**：
- **ROUGE-1**：基于单词的召回率
- **ROUGE-2**：基于 2-gram 的召回率
- **ROUGE-L**：基于最长公共子序列的召回率

**意义**：
- 主要用于摘要任务
- 衡量生成文本覆盖参考文本的程度

### 5. BERTScore

**定义**：基于 BERT 嵌入的语义相似度

**公式**：
```
BERTScore = (1/N) * Σ(max(sim(gen_i, ref_j)))
其中 sim 是 BERT 嵌入的余弦相似度
```

**意义**：
- 考虑语义相似度，而不仅仅是表面相似度
- 更适合评估 RAG 系统
- **BERTScore 越高**：生成文本与参考文本语义越相似

## 四、端到端评估指标

### 1. 答案准确性（Answer Accuracy）

**定义**：生成的回答是否正确的比例

**评估方式**：
- **人工评估**：人工判断回答是否正确
- **自动评估**：与标准答案对比（如果有）

**意义**：
- 最直观的评估指标
- 综合反映检索和生成的质量

### 2. 用户满意度（User Satisfaction）

**定义**：用户对系统回答的满意程度

**评估方式**：
- **用户评分**：1-5 分
- **用户反馈**：正面/负面反馈
- **任务完成率**：用户是否通过系统完成了任务

**意义**：
- 最贴近实际应用的指标
- 综合反映系统的可用性

## 五、评估数据集

### 1. 标准数据集

**检索评估数据集**：
- **MS MARCO**：大规模检索数据集
- **Natural Questions**：自然问题数据集
- **SQuAD**：阅读理解数据集

**RAG 评估数据集**：
- **RAGAS**：专门用于 RAG 系统评估
- **BEIR**：信息检索基准测试
- **MTEB**：多任务嵌入基准测试

### 2. 自定义评估数据集

**构建方式**：
1. 收集真实问题和对应的文档
2. 人工标注相关文档块
3. 准备标准答案（可选）

**示例**：
```json
{
  "question": "如何做西红柿鸡蛋？",
  "relevant_chunks": ["chunk_1", "chunk_2"],
  "ground_truth_answer": "西红柿鸡蛋的做法是..."
}
```

## 六、评估实践建议

### 1. 评估流程

```
1. 准备评估数据集
   ↓
2. 运行 RAG 系统，获取检索结果和生成回答
   ↓
3. 计算检索指标（Recall, Precision, MRR, NDCG）
   ↓
4. 计算生成指标（Faithfulness, Relevance, BERTScore）
   ↓
5. 综合评估（Answer Accuracy, User Satisfaction）
```

### 2. 关键指标选择

**检索阶段**：
- **必选**：Recall@k（k 通常为 5 或 10）
- **推荐**：MRR、NDCG@k

**生成阶段**：
- **必选**：Faithfulness（防止幻觉）
- **推荐**：Relevance、BERTScore

**端到端**：
- **必选**：Answer Accuracy
- **推荐**：User Satisfaction

### 3. 评估基准

**检索阶段**：
- **Recall@5 > 0.8**：优秀
- **Recall@5 > 0.6**：良好
- **Recall@5 < 0.6**：需要改进

**生成阶段**：
- **Faithfulness > 0.9**：优秀
- **Faithfulness > 0.7**：良好
- **Faithfulness < 0.7**：需要改进

## 七、面试常见问题

### Q1: RAG 系统最重要的评估指标是什么？

**A**: 
1. **召回率（Recall）**：确保不遗漏相关信息
2. **忠实度（Faithfulness）**：防止模型幻觉
3. **答案准确性（Answer Accuracy）**：综合反映系统质量

### Q2: 如何提高召回率？

**A**: 
1. **增加 top_k**：检索更多文档块
2. **优化 embedding 模型**：使用更好的语义表示
3. **混合检索**：结合向量检索和关键词检索
4. **查询扩展**：扩展用户问题，增加检索覆盖面
5. **重排序（Reranking）**：使用更精确的模型对检索结果重排序

### Q3: 如何评估 RAG 系统是否产生了幻觉？

**A**: 
1. **忠实度（Faithfulness）评估**：判断回答是否可以从检索到的文档中推断出来
2. **人工检查**：人工验证回答中的事实是否在文档中
3. **自动检测**：使用 LLM 判断回答是否基于文档

### Q4: 召回率和精确率的权衡？

**A**: 
- **高召回率 + 低精确率**：适合需要全面信息的场景（如：研究、分析）
- **低召回率 + 高精确率**：适合需要精确答案的场景（如：事实查询）
- **理想状态**：高召回率 + 高精确率（通过优化检索策略实现）

### Q5: 如何构建评估数据集？

**A**: 
1. **收集真实问题**：从用户日志、FAQ 等收集
2. **标注相关文档**：人工标注每个问题对应的相关文档块
3. **准备标准答案**（可选）：为每个问题准备标准答案
4. **划分数据集**：训练集、验证集、测试集

## 八、总结

| 评估层次 | 关键指标 | 重要性 |
|---------|---------|--------|
| **检索阶段** | Recall@k | ⭐⭐⭐⭐⭐ |
| **检索阶段** | MRR, NDCG@k | ⭐⭐⭐⭐ |
| **生成阶段** | Faithfulness | ⭐⭐⭐⭐⭐ |
| **生成阶段** | Relevance | ⭐⭐⭐⭐ |
| **端到端** | Answer Accuracy | ⭐⭐⭐⭐⭐ |

**核心原则**：
- **召回率优先**：确保不遗漏信息
- **忠实度优先**：防止幻觉
- **综合评估**：结合多个指标全面评估

