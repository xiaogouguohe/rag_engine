#!/usr/bin/env python3
"""
è°ƒè¯• RAGAS è°ƒç”¨ LLM API æ—¶çš„å‚æ•°ï¼ˆæ–¹æ³• 2ï¼šä½¿ç”¨ LangChain å›è°ƒï¼‰
"""
import sys
from pathlib import Path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

import json
import warnings
warnings.filterwarnings('ignore', category=DeprecationWarning)

from langchain_openai import ChatOpenAI
from langchain_core.callbacks import BaseCallbackHandler
from ragas.llms import LangchainLLMWrapper
from ragas.metrics._answer_relevance import ResponseRelevanceInput, ResponseRelevancePrompt
from config.config import AppConfig

class APICallDebugHandler(BaseCallbackHandler):
    """å›è°ƒå¤„ç†å™¨ï¼Œç”¨äºè®°å½• API è°ƒç”¨"""
    
    def __init__(self):
        self.api_calls = []
        self.current_call = {}
    
    def on_llm_start(self, serialized, prompts, **kwargs):
        """LLM å¼€å§‹è°ƒç”¨æ—¶"""
        print("\n" + "=" * 60)
        print("ğŸ” LLM å¼€å§‹è°ƒç”¨")
        print("=" * 60)
        print(f"ğŸ“‹ serialized: {serialized}")
        print(f"ğŸ“‹ prompts æ•°é‡: {len(prompts)}")
        
        self.current_call = {
            "event": "llm_start",
            "prompts": prompts,
            "kwargs": kwargs,
        }
        
        # æ£€æŸ¥æ¯ä¸ª prompt
        for i, prompt in enumerate(prompts):
            print(f"\nğŸ“ Prompt {i+1}:")
            print(f"  - ç±»å‹: {type(prompt)}")
            print(f"  - å€¼: {repr(str(prompt)[:200])}")
            
            if isinstance(prompt, list):
                print(f"  - æ˜¯åˆ—è¡¨ï¼Œé•¿åº¦: {len(prompt)}")
                for j, item in enumerate(prompt):
                    print(f"    é¡¹ç›® {j}: ç±»å‹={type(item)}, å€¼={repr(str(item)[:100])}")
    
    def on_llm_end(self, response, **kwargs):
        """LLM è°ƒç”¨ç»“æŸæ—¶"""
        print("\n" + "=" * 60)
        print("âœ… LLM è°ƒç”¨ç»“æŸ")
        print("=" * 60)
        print(f"ğŸ“‹ response ç±»å‹: {type(response)}")
        
        self.current_call["event"] = "llm_end"
        self.current_call["response"] = str(response)[:500]  # åªä¿å­˜å‰500å­—ç¬¦
        self.api_calls.append(self.current_call.copy())
    
    def on_llm_error(self, error, **kwargs):
        """LLM è°ƒç”¨å‡ºé”™æ—¶"""
        print("\n" + "=" * 60)
        print("âŒ LLM è°ƒç”¨å‡ºé”™")
        print("=" * 60)
        print(f"ğŸ“‹ error: {error}")
        print(f"ğŸ“‹ error ç±»å‹: {type(error)}")
        
        self.current_call["event"] = "llm_error"
        self.current_call["error"] = str(error)
        self.api_calls.append(self.current_call.copy())

def debug_with_callbacks():
    """ä½¿ç”¨å›è°ƒå‡½æ•°è°ƒè¯•"""
    print("=" * 60)
    print("è°ƒè¯• RAGAS è°ƒç”¨ LLM APIï¼ˆä½¿ç”¨å›è°ƒå‡½æ•°ï¼‰")
    print("=" * 60)
    print()
    
    # åŠ è½½é…ç½®
    app_config = AppConfig.load()
    
    # åˆ›å»ºå›è°ƒå¤„ç†å™¨
    debug_handler = APICallDebugHandler()
    
    # åˆ›å»º LangChain LLM
    print("1. åˆ›å»º LangChain LLMï¼ˆå¸¦å›è°ƒï¼‰...")
    langchain_llm = ChatOpenAI(
        model=app_config.llm.model,
        api_key=app_config.llm.api_key,
        base_url=app_config.llm.base_url,
        temperature=0.1,
        timeout=120.0,
        max_retries=3,
        callbacks=[debug_handler],
    )
    print(f"   âœ… LangChain LLM åˆ›å»ºæˆåŠŸ")
    print()
    
    # åˆ›å»º RAGAS LLM Wrapper
    print("2. åˆ›å»º RAGAS LLM Wrapper...")
    ragas_llm = LangchainLLMWrapper(langchain_llm)
    print(f"   âœ… RAGAS LLM Wrapper åˆ›å»ºæˆåŠŸ")
    print()
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    print("3. åˆ›å»ºæµ‹è¯•æ•°æ®...")
    test_answer = "æ ¹æ®æ–‡æ¡£ç‰‡æ®µï¼Œå¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤ç”¨å’–å–±çƒ¹é¥ªé’èŸ¹ã€‚"
    prompt_input = ResponseRelevanceInput(response=test_answer)
    print(f"   âœ… æµ‹è¯•æ•°æ®åˆ›å»ºæˆåŠŸ")
    print()
    
    # åˆ›å»º Prompt
    print("4. åˆ›å»º Prompt...")
    prompt = ResponseRelevancePrompt()
    print(f"   âœ… Prompt åˆ›å»ºæˆåŠŸ")
    print()
    
    # æµ‹è¯•ç”Ÿæˆ
    print("5. æµ‹è¯•ç”Ÿæˆé—®é¢˜ï¼ˆstrictness=1ï¼‰...")
    print("-" * 60)
    
    import asyncio
    async def test_generate():
        try:
            result = await prompt.generate_multiple(
                llm=ragas_llm,
                data=prompt_input,
                n=1,
                callbacks=[debug_handler],
            )
            print(f"\nâœ… ç”ŸæˆæˆåŠŸï¼Œç»“æœæ•°é‡: {len(result)}")
            return result
        except Exception as e:
            print(f"\nâŒ ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    result = asyncio.run(test_generate())
    print()
    
    # ä¿å­˜è°ƒç”¨è®°å½•ï¼ˆæ¸…ç†ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡ï¼‰
    print("6. ä¿å­˜ API è°ƒç”¨è®°å½•...")
    output_file = Path("ragas_api_calls_callbacks.json")
    
    # æ¸…ç†ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡
    cleaned_calls = []
    for call in debug_handler.api_calls:
        cleaned_call = {}
        for key, value in call.items():
            try:
                json.dumps(value)  # æµ‹è¯•æ˜¯å¦å¯ä»¥åºåˆ—åŒ–
                cleaned_call[key] = value
            except (TypeError, ValueError):
                cleaned_call[key] = str(value)  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        cleaned_calls.append(cleaned_call)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(cleaned_calls, f, ensure_ascii=False, indent=2)
    print(f"   âœ… è°ƒç”¨è®°å½•å·²ä¿å­˜åˆ°: {output_file}")
    print()
    
    print("=" * 60)
    print("è°ƒè¯•å®Œæˆ")
    print("=" * 60)

if __name__ == "__main__":
    debug_with_callbacks()

