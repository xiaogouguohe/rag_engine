from __future__ import annotations

"""
RAG å¼•æ“
--------

æ•´åˆæ‰€æœ‰æ¨¡å—ï¼Œå®ç°å®Œæ•´çš„ RAG æµç¨‹ï¼š
1. æ–‡æ¡£å¤„ç†æµç¨‹ï¼šæ–‡æ¡£ â†’ è§£æ â†’ åˆ†å— â†’ å‘é‡åŒ– â†’ å­˜å‚¨
2. é—®ç­”æµç¨‹ï¼šé—®é¢˜ â†’ å‘é‡åŒ– â†’ æ£€ç´¢ â†’ ç”Ÿæˆå›ç­”

å‚è€ƒ RAGFlow çš„è®¾è®¡æ€è·¯ï¼Œä½†å¤§å¹…ç®€åŒ–å®ç°ã€‚
"""

from pathlib import Path
from typing import List, Dict, Any, Optional
import uuid

from config import AppConfig
from document import (
    ParserFactory,
    TextChunker,
    MetadataEnhancer,
    DataPreparationModule,
)
from embedding import EmbeddingClient
from llm import LLMClient
from vector_store import VectorStore


class RAGEngine:
    """
    RAG å¼•æ“æ ¸å¿ƒç±»ï¼Œæ•´åˆæ‰€æœ‰æ¨¡å—ã€‚
    
    åŠŸèƒ½ï¼š
    - æ–‡æ¡£å¤„ç†ï¼šè§£æã€åˆ†å—ã€å‘é‡åŒ–ã€å­˜å‚¨
    - é—®ç­”ï¼šæ£€ç´¢ã€ç”Ÿæˆå›ç­”
    """
    
    def __init__(
        self,
        kb_id: str,
        config: Optional[AppConfig] = None,
        chunk_size: int = 500,
        chunk_overlap: int = 50,
        use_markdown_header_split: bool = True,
        metadata_enhancer: Optional[MetadataEnhancer] = None,
    ):
        """
        åˆå§‹åŒ– RAG å¼•æ“ã€‚
        
        Args:
            kb_id: çŸ¥è¯†åº“ ID
            config: åº”ç”¨é…ç½®ï¼ˆå¦‚æœä¸æä¾›ï¼Œåˆ™è‡ªåŠ¨åŠ è½½ï¼‰
            chunk_size: åˆ†å—å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
            chunk_overlap: åˆ†å—é‡å å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
            use_markdown_header_split: æ˜¯å¦å¯¹ Markdown ä½¿ç”¨æ ‡é¢˜åˆ†å‰²ï¼ˆå‚è€ƒ C8ï¼‰
            metadata_enhancer: å…ƒæ•°æ®å¢å¼ºå™¨ï¼ˆå¯é€‰ï¼‰
        """
        self.kb_id = kb_id
        
        # åŠ è½½é…ç½®
        if config is None:
            config = AppConfig.load()
        self.config = config
        
        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        self.data_module = DataPreparationModule(
            metadata_enhancer=metadata_enhancer,
            use_markdown_header_split=use_markdown_header_split,
        )
        self.embedding_client = EmbeddingClient.from_config(config)
        self.llm_client = LLMClient.from_config(config)
        self.vector_store = VectorStore(storage_path=config.storage_path)
    
    def ingest_document(
        self,
        file_path: str | Path,
        doc_id: Optional[str] = None,
        verbose: bool = True,
    ) -> Dict[str, Any]:
        """
        å¤„ç†æ–‡æ¡£ï¼šè§£æ â†’ åˆ†å— â†’ å‘é‡åŒ– â†’ å­˜å‚¨ï¼ˆå‚è€ƒ C8 çš„å®ç°ï¼‰ã€‚
        
        Args:
            file_path: æ–‡æ¡£æ–‡ä»¶è·¯å¾„
            doc_id: æ–‡æ¡£ IDï¼ˆå¦‚æœä¸æä¾›ï¼Œåˆ™è‡ªåŠ¨ç”Ÿæˆï¼‰
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†è¿›åº¦
        
        Returns:
            å¤„ç†ç»“æœï¼š
            {
                "doc_id": str,
                "chunks_count": int,
                "status": "success"
            }
        """
        import time
        
        file_path = Path(file_path)
        
        if not file_path.exists():
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        if verbose:
            print(f"  ğŸ“„ æ–‡ä»¶: {file_path.name} ({file_path.stat().st_size} å­—èŠ‚)")
        
        # 1. åŠ è½½æ–‡æ¡£ï¼ˆä¼šè‡ªåŠ¨è§£æå’Œåˆ†å—ï¼Œå¦‚æœæ˜¯ Markdown ä¸”å¯ç”¨æ ‡é¢˜åˆ†å‰²ï¼‰
        if verbose:
            print(f"  â³ æ­¥éª¤ 1/6: åŠ è½½å’Œè§£ææ–‡æ¡£...")
        start_time = time.time()
        
        self.data_module.load_documents([file_path], enhance_metadata=True)
        
        load_time = time.time() - start_time
        if verbose:
            print(f"  âœ… æ­¥éª¤ 1 å®Œæˆï¼Œè€—æ—¶: {load_time:.2f} ç§’")
        
        # 2. å¦‚æœè¿˜æ²¡æœ‰åˆ†å—ï¼Œè¿›è¡Œåˆ†å—
        if verbose:
            print(f"  â³ æ­¥éª¤ 2/6: æ–‡æ¡£åˆ†å—...")
        start_time = time.time()
        
        if not self.data_module.chunks:
            self.data_module.chunk_documents()
        
        chunk_time = time.time() - start_time
        if verbose:
            print(f"  âœ… æ­¥éª¤ 2 å®Œæˆï¼Œè€—æ—¶: {chunk_time:.2f} ç§’")
        
        # 3. è·å–è¯¥æ–‡æ¡£çš„å—ï¼ˆé€šè¿‡ parent_id åŒ¹é…ï¼‰
        if verbose:
            print(f"  â³ æ­¥éª¤ 3/6: æå–æ–‡æ¡£å—...")
        start_time = time.time()
        
        # æ‰¾åˆ°åˆšåŠ è½½çš„æ–‡æ¡£
        parent_doc = None
        for doc in self.data_module.documents:
            if str(file_path) in doc.metadata.get("source", ""):
                parent_doc = doc
                break
        
        if not parent_doc:
            raise ValueError(f"æ–‡æ¡£åŠ è½½å¤±è´¥: {file_path}")
        
        parent_id = parent_doc.metadata.get("parent_id")
        doc_chunks = [
            chunk for chunk in self.data_module.chunks
            if chunk.metadata.get("parent_id") == parent_id
        ]
        
        if not doc_chunks:
            raise ValueError(f"æ–‡æ¡£åˆ†å—å¤±è´¥: {file_path}")
        
        extract_time = time.time() - start_time
        if verbose:
            print(f"  âœ… æ­¥éª¤ 3 å®Œæˆï¼Œæ‰¾åˆ° {len(doc_chunks)} ä¸ªå—ï¼Œè€—æ—¶: {extract_time:.2f} ç§’")
        
        # 4. æå–æ–‡æœ¬å’Œå…ƒæ•°æ®
        if verbose:
            print(f"  â³ æ­¥éª¤ 4/6: å‡†å¤‡å‘é‡åŒ–æ•°æ®...")
        start_time = time.time()
        
        texts = [chunk.page_content for chunk in doc_chunks]
        metadatas = [chunk.metadata for chunk in doc_chunks]
        
        prep_time = time.time() - start_time
        if verbose:
            total_text_len = sum(len(t) for t in texts)
            print(f"  âœ… æ­¥éª¤ 4 å®Œæˆï¼Œæ€»æ–‡æœ¬é•¿åº¦: {total_text_len} å­—ç¬¦ï¼Œè€—æ—¶: {prep_time:.2f} ç§’")
        
        # 5. å‘é‡åŒ–ï¼ˆæœ€å¯èƒ½å¡ä½çš„åœ°æ–¹ï¼‰
        if verbose:
            print(f"  â³ æ­¥éª¤ 5/6: è°ƒç”¨å‘é‡åŒ– APIï¼ˆ{len(texts)} ä¸ªæ–‡æœ¬å—ï¼‰...")
            print(f"     è¿™å¯èƒ½éœ€è¦å‡ ç§’åˆ°å‡ åç§’ï¼Œå–å†³äºç½‘ç»œå’Œ API å“åº”é€Ÿåº¦...")
        start_time = time.time()
        
        try:
            vectors = self.embedding_client.embed_texts(texts, verbose=verbose)
            embed_time = time.time() - start_time
            if verbose:
                print(f"  âœ… æ­¥éª¤ 5 å®Œæˆï¼Œç”Ÿæˆ {len(vectors)} ä¸ªå‘é‡ï¼Œè€—æ—¶: {embed_time:.2f} ç§’")
        except Exception as e:
            embed_time = time.time() - start_time
            if verbose:
                print(f"  âŒ æ­¥éª¤ 5 å¤±è´¥ï¼Œè€—æ—¶: {embed_time:.2f} ç§’")
                print(f"     é”™è¯¯è¯¦æƒ…: {type(e).__name__}: {e}")
            raise RuntimeError(f"å‘é‡åŒ–å¤±è´¥: {e}") from e
        
        # 6. å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
        if verbose:
            print(f"  â³ æ­¥éª¤ 6/6: å†™å…¥å‘é‡æ•°æ®åº“ï¼ˆMilvusï¼‰...")
        start_time = time.time()
        
        try:
            chunk_ids = self.vector_store.add_texts(
                kb_id=self.kb_id,
                texts=texts,
                vectors=vectors,
                metadatas=metadatas,
            )
            store_time = time.time() - start_time
            if verbose:
                print(f"  âœ… æ­¥éª¤ 6 å®Œæˆï¼Œå†™å…¥ {len(chunk_ids)} ä¸ªå‘é‡ï¼Œè€—æ—¶: {store_time:.2f} ç§’")
        except Exception as e:
            store_time = time.time() - start_time
            if verbose:
                print(f"  âŒ æ­¥éª¤ 6 å¤±è´¥ï¼Œè€—æ—¶: {store_time:.2f} ç§’")
            raise RuntimeError(f"å‘é‡æ•°æ®åº“å†™å…¥å¤±è´¥: {e}") from e
        
        total_time = load_time + chunk_time + extract_time + prep_time + embed_time + store_time
        if verbose:
            print(f"  ğŸ“Š æ€»è€—æ—¶: {total_time:.2f} ç§’")
        
        return {
            "doc_id": parent_id,
            "chunks_count": len(doc_chunks),
            "chunk_ids": chunk_ids,
            "status": "success",
        }
    
    def query(
        self,
        question: str,
        top_k: int = 5,
        similarity_threshold: float = 0.0,
        system_prompt: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        é—®ç­”æµç¨‹ï¼šé—®é¢˜ â†’ å‘é‡åŒ– â†’ æ£€ç´¢ â†’ ç”Ÿæˆå›ç­”
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            top_k: æ£€ç´¢ top-k ä¸ªç›¸å…³å—
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆä½äºæ­¤å€¼çš„å—å°†è¢«è¿‡æ»¤ï¼‰
            system_prompt: ç³»ç»Ÿæç¤ºè¯ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            å›ç­”ç»“æœï¼š
            {
                "answer": str,  # LLM ç”Ÿæˆçš„å›ç­”
                "sources": List[Dict],  # æ£€ç´¢åˆ°çš„ç›¸å…³å—
                "query": str,  # åŸå§‹é—®é¢˜
            }
        """
        if not question.strip():
            raise ValueError("é—®é¢˜ä¸èƒ½ä¸ºç©º")
        
        # 1. é—®é¢˜å‘é‡åŒ–
        query_vectors = self.embedding_client.embed_texts([question])
        query_vector = query_vectors[0]
        
        # 2. æ£€ç´¢ç›¸å…³å—
        search_results = self.vector_store.search(
            kb_id=self.kb_id,
            query_vector=query_vector,
            top_k=top_k,
        )
        
        # 3. è¿‡æ»¤ä½ç›¸ä¼¼åº¦çš„ç»“æœ
        filtered_results = [
            (score, metadata) for score, metadata in search_results
            if score >= similarity_threshold
        ]
        
        if not filtered_results:
            return {
                "answer": "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚",
                "sources": [],
                "query": question,
            }
        
        # 4. æ„å»ºä¸Šä¸‹æ–‡
        context_chunks = []
        for score, metadata in filtered_results:
            context_chunks.append({
                "text": metadata.text,
                "score": score,
                "doc_id": metadata.doc_id,
                "chunk_id": metadata.chunk_id,
            })
        
        # 5. æ‹¼æ¥ä¸Šä¸‹æ–‡å’Œé—®é¢˜
        context = "\n\n".join([
            f"[æ–‡æ¡£ç‰‡æ®µ {i+1}]\n{chunk['text']}"
            for i, chunk in enumerate(context_chunks)
        ])
        
        # 6. æ„å»ºæç¤ºè¯
        if system_prompt is None:
            system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ AI åŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„æ–‡æ¡£ç‰‡æ®µå›ç­”é—®é¢˜ã€‚
å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯šå®åœ°è¯´ä¸çŸ¥é“ã€‚"""
        
        prompt = f"""åŸºäºä»¥ä¸‹æ–‡æ¡£ç‰‡æ®µå›ç­”é—®é¢˜ï¼š

{context}

é—®é¢˜ï¼š{question}

è¯·åŸºäºä¸Šè¿°æ–‡æ¡£ç‰‡æ®µå›ç­”é—®é¢˜ï¼Œå¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´æ˜ã€‚"""
        
        # 7. ç”Ÿæˆå›ç­”
        answer = self.llm_client.generate(
            prompt=prompt,
            system_prompt=system_prompt,
            temperature=0.1,
        )
        
        return {
            "answer": answer,
            "sources": context_chunks,
            # å‘åå…¼å®¹ï¼šæœ‰äº›è¯„ä¼°è„šæœ¬æˆ–ä¸Šå±‚ä¼šç”¨ chunks è¡¨ç¤ºæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡å—
            "chunks": context_chunks,
            "query": question,
        }
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯"""
        return self.vector_store.get_stats(self.kb_id)
    
    def delete_knowledge_base(self):
        """åˆ é™¤æ•´ä¸ªçŸ¥è¯†åº“"""
        self.vector_store.delete_knowledge_base(self.kb_id)


__all__ = ["RAGEngine"]

