from __future__ import annotations

"""
RAG å¼•æ“
--------

æ•´åˆæ‰€æœ‰æ¨¡å—ï¼Œå®ç°å®Œæ•´çš„ RAG æµç¨‹ï¼š
1. æ–‡æ¡£å¤„ç†æµç¨‹ï¼šæ–‡æ¡£ â†’ è§£æ â†’ åˆ†å— â†’ å‘é‡åŒ– â†’ å­˜å‚¨
2. é—®ç­”æµç¨‹ï¼šé—®é¢˜ â†’ å‘é‡åŒ– â†’ æ£€ç´¢ â†’ ç”Ÿæˆå›ç­”

å‚è€ƒ RAGFlow çš„è®¾è®¡æ€è·¯ï¼Œä½†å¤§å¹…ç®€åŒ–å®ç°ã€‚
"""

from pathlib import Path
from typing import List, Dict, Any, Optional
import uuid
import numpy as np

from config import AppConfig
from document import (
    ParserFactory,
    TextChunker,
    MetadataEnhancer,
    DataPreparationModule,
)
from embedding import EmbeddingClient
from llm import LLMClient
from vector_store import VectorStore


class RAGEngine:
    """
    RAG å¼•æ“æ ¸å¿ƒç±»ï¼Œæ•´åˆæ‰€æœ‰æ¨¡å—ã€‚
    
    åŠŸèƒ½ï¼š
    - æ–‡æ¡£å¤„ç†ï¼šè§£æã€åˆ†å—ã€å‘é‡åŒ–ã€å­˜å‚¨
    - é—®ç­”ï¼šæ£€ç´¢ã€ç”Ÿæˆå›ç­”
    """
    
    def __init__(
        self,
        kb_id: str,
        config: Optional[AppConfig] = None,
        chunk_size: int = 500,
        chunk_overlap: int = 50,
        use_markdown_header_split: bool = True,
        metadata_enhancer: Optional[MetadataEnhancer] = None,
    ):
        """
        åˆå§‹åŒ– RAG å¼•æ“ã€‚
        
        Args:
            kb_id: çŸ¥è¯†åº“ ID
            config: åº”ç”¨é…ç½®ï¼ˆå¦‚æœä¸æä¾›ï¼Œåˆ™è‡ªåŠ¨åŠ è½½ï¼‰
            chunk_size: åˆ†å—å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
            chunk_overlap: åˆ†å—é‡å å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
            use_markdown_header_split: æ˜¯å¦å¯¹ Markdown ä½¿ç”¨æ ‡é¢˜åˆ†å‰²ï¼ˆå‚è€ƒ C8ï¼‰
            metadata_enhancer: å…ƒæ•°æ®å¢å¼ºå™¨ï¼ˆå¯é€‰ï¼‰
        """
        self.kb_id = kb_id
        
        # åŠ è½½é…ç½®
        if config is None:
            config = AppConfig.load()
        self.config = config
        
        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        self.data_module = DataPreparationModule(
            metadata_enhancer=metadata_enhancer,
            use_markdown_header_split=use_markdown_header_split,
        )
        self.embedding_client = EmbeddingClient.from_config(config)
        self.llm_client = LLMClient.from_config(config)
        self.vector_store = VectorStore(storage_path=config.storage_path)
        
        # è®°å½•è¯¥çŸ¥è¯†åº“çš„ç‰¹å®šé…ç½®
        self.kb_config = None
        if self.config.knowledge_bases:
            for kb in self.config.knowledge_bases:
                if kb.kb_id == kb_id:
                    self.kb_config = kb
                    break
        
        # è®¾ç½®é»˜è®¤æ£€ç´¢æ•°é‡
        self.default_top_k = self.kb_config.top_k if self.kb_config else 4
    
    def ingest_document(
        self,
        file_path: str | Path,
        doc_id: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        å¤„ç†å•ç¯‡æ–‡æ¡£ï¼šè§£æ â†’ åˆ†å— â†’ å‘é‡åŒ– â†’ å­˜å‚¨ã€‚
        """
        file_path = Path(file_path)
        
        if not file_path.exists():
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        # 1. æ¸…ç†ä¹‹å‰çš„çŠ¶æ€ï¼Œç¡®ä¿åªå¤„ç†å½“å‰æ–‡æ¡£
        self.data_module.documents = []
        self.data_module.chunks = []
        
        # 2. åŠ è½½æ–‡æ¡£
        self.data_module.load_documents([file_path], enhance_metadata=True)
        
        # 3. è¿›è¡Œåˆ†å—
        if not self.data_module.chunks:
            self.data_module.chunk_documents()
        
        # 4. è·å–è¯¥æ–‡æ¡£çš„å—
        parent_doc = None
        for doc in self.data_module.documents:
            if str(file_path) in doc.metadata.get("source", ""):
                parent_doc = doc
                break
        
        if not parent_doc:
            raise ValueError(f"æ–‡æ¡£åŠ è½½å¤±è´¥: {file_path}")
        
        parent_id = parent_doc.metadata.get("parent_id")
        doc_chunks = [
            chunk for chunk in self.data_module.chunks
            if chunk.metadata.get("parent_id") == parent_id
        ]
        
        if not doc_chunks:
            raise ValueError(f"æ–‡æ¡£åˆ†å—å¤±è´¥: {file_path}")
        
        # 5. æå–æ–‡æœ¬å’Œå…ƒæ•°æ®
        texts = [chunk.page_content for chunk in doc_chunks]
        metadatas = [chunk.metadata for chunk in doc_chunks]
        
        # 6. å‘é‡åŒ– (æ ¹æ®é…ç½®å†³å®šæ˜¯å¦ç”Ÿæˆå¤šç§å‘é‡)
        use_sparse = self.kb_config.use_sparse if self.kb_config else False
        # å­˜å‚¨æ—¶ä¸åŒ…å« multi_vector
        
        emb_results = self.embedding_client.embed_texts(
            texts, 
            return_sparse=use_sparse,
            return_multi=False
        )
        
        # 7. å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
        chunk_ids = self.vector_store.add_texts(
            kb_id=self.kb_id,
            texts=texts,
            vectors=emb_results["dense_vecs"],
            metadatas=metadatas,
            sparse_vectors=emb_results.get("sparse_vecs")
        )
        
        return {
            "doc_id": parent_id,
            "chunks_count": len(doc_chunks),
            "chunk_ids": chunk_ids,
            "status": "success",
        }

    def ingest_directory(
        self,
        dir_path: str | Path,
        pattern: str = "*.md",
        verbose: bool = True,
    ) -> Dict[str, Any]:
        """
        æ‰¹é‡å¤„ç†ç›®å½•ä¸‹çš„æ–‡æ¡£ã€‚
        
        Args:
            dir_path: ç›®å½•è·¯å¾„
            pattern: æ–‡ä»¶åŒ¹é…æ¨¡å¼
            verbose: æ˜¯å¦æ‰“å°è¿›åº¦
            
        Returns:
            å¤„ç†ç»Ÿè®¡ç»“æœ
        """
        dir_path = Path(dir_path)
        if not dir_path.exists() or not dir_path.is_dir():
            raise FileNotFoundError(f"ç›®å½•ä¸å­˜åœ¨æˆ–ä¸æ˜¯ç›®å½•: {dir_path}")

        files = sorted(list(dir_path.rglob(pattern)))
        if not files:
            return {
                "total_files": 0,
                "success_count": 0,
                "fail_count": 0,
                "total_chunks": 0,
            }

        if verbose:
            print(f"å¼€å§‹åŠ è½½ç›®å½•: {dir_path} (æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶)")
            print("-" * 40)

        success_count = 0
        fail_count = 0
        total_chunks = 0

        for i, file_path in enumerate(files, 1):
            if verbose:
                try:
                    rel_path = file_path.relative_to(dir_path)
                except ValueError:
                    rel_path = file_path.name
                print(f"[{i}/{len(files)}] å¤„ç†: {rel_path}", end=" ... ", flush=True)
            
            try:
                result = self.ingest_document(file_path)
                success_count += 1
                total_chunks += result["chunks_count"]
                if verbose:
                    print(f"âœ… ({result['chunks_count']} å—)")
            except Exception as e:
                fail_count += 1
                if verbose:
                    print(f"âŒ å¤±è´¥: {e}")

        return {
            "total_files": len(files),
            "success_count": success_count,
            "fail_count": fail_count,
            "total_chunks": total_chunks,
            "status": "completed"
        }
    
    def query(
        self,
        question: str,
        top_k: Optional[int] = None,
        similarity_threshold: float = 0.0,
        system_prompt: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        é—®ç­”æµç¨‹ï¼šæ”¯æŒå¯†é›†ã€ç¨€ç–ã€å¤šå‘é‡æ£€ç´¢ã€‚
        """
        if not question.strip():
            raise ValueError("é—®é¢˜ä¸èƒ½ä¸ºç©º")
        
        # 0. è·å–é…ç½®
        final_top_k = top_k if top_k is not None else self.default_top_k
        use_sparse = self.kb_config.use_sparse if self.kb_config else False
        use_multi = self.kb_config.use_multi_vector if self.kb_config else False
        
        # 1. é—®é¢˜å‘é‡åŒ–
        emb_results = self.embedding_client.embed_texts(
            [question], 
            return_sparse=use_sparse,
            return_multi=use_multi
        )
        query_dense = emb_results["dense_vecs"][0]
        query_sparse = emb_results.get("sparse_vecs", [None])[0]
        query_multi = emb_results.get("multi_vecs", [None])[0]
        
        # 2. æ£€ç´¢ç›¸å…³å— (æ··åˆæ£€ç´¢: Dense + Sparse)
        search_results = self.vector_store.search(
            kb_id=self.kb_id,
            query_vector=query_dense,
            top_k=final_top_k * 3 if use_multi else final_top_k, # å¦‚æœæœ‰ç²¾æ’ï¼Œå…ˆå¤šæç‚¹
            query_sparse_vector=query_sparse,
        )
        
        # 3. å¦‚æœå¯ç”¨å¤šå‘é‡ç²¾æ’ (ColBERT Online Rerank)
        if use_multi and query_multi is not None and search_results:
            print(f"     ğŸ¯ æ­£åœ¨å¯¹ {len(search_results)} ä¸ªå€™é€‰ç‰‡æ®µè¿›è¡Œåœ¨çº¿å¤šå‘é‡ç²¾æ’ (ColBERT)...")
            
            # è·å–å€™é€‰ç‰‡æ®µçš„åŸå§‹æ–‡æœ¬
            candidate_texts = [res[1].text for res in search_results]
            
            # ç°åœºè®¡ç®—å€™é€‰ç‰‡æ®µçš„å¤šå‘é‡ (Online Encoding)
            # æ³¨æ„ï¼šè¿™é‡Œåªè®¡ç®—å‡ åä¸ªç‰‡æ®µï¼Œé€Ÿåº¦ä¼šå¾ˆå¿«
            candidate_emb = self.embedding_client.embed_texts(
                candidate_texts, 
                return_dense=False, 
                return_sparse=False, 
                return_multi=True
            )
            candidate_multi_vecs = candidate_emb.get("multi_vecs")
            
            reranked_results = []
            if candidate_multi_vecs is not None:
                for i, (score, metadata) in enumerate(search_results):
                    doc_multi = candidate_multi_vecs[i]
                    
                    # è®¡ç®— ColBERT MaxSim åˆ†æ•°
                    # query_multi: [q_len, dim], doc_multi: [d_len, dim]
                    sim_matrix = np.matmul(query_multi, doc_multi.T)
                    max_sim_score = np.mean(np.max(sim_matrix, axis=1))
                    
                    # èåˆåˆ†æ•°
                    final_score = score * 0.3 + max_sim_score * 0.7
                    reranked_results.append((final_score, metadata))
                
                # é‡æ–°æ’åºå¹¶å–æœ€ç»ˆ top_k
                reranked_results.sort(key=lambda x: x[0], reverse=True)
                search_results = reranked_results[:final_top_k]
            else:
                search_results = search_results[:final_top_k]
        else:
            # å¦‚æœæ²¡å¼€ç²¾æ’ï¼Œç›´æ¥å– top_k
            search_results = search_results[:final_top_k]
        
        # 4. è¿‡æ»¤ä½ç›¸ä¼¼åº¦çš„ç»“æœ
        filtered_results = [
            (score, metadata) for score, metadata in search_results
            if score >= similarity_threshold
        ]
        
        if not filtered_results:
            return {
                "answer": "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚",
                "sources": [],
                "query": question,
            }
        
        # 4. æ„å»ºä¸Šä¸‹æ–‡
        context_chunks = []
        for score, metadata in filtered_results:
            context_chunks.append({
                "text": metadata.text,
                "score": score,
                "doc_id": metadata.doc_id,
                "chunk_id": metadata.chunk_id,
            })
        
        # 5. æ‹¼æ¥ä¸Šä¸‹æ–‡å’Œé—®é¢˜
        context = "\n\n".join([
            f"[æ–‡æ¡£ç‰‡æ®µ {i+1}]\n{chunk['text']}"
            for i, chunk in enumerate(context_chunks)
        ])
        
        # 6. æ„å»ºæç¤ºè¯
        if system_prompt is None:
            system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ AI åŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„æ–‡æ¡£ç‰‡æ®µå›ç­”é—®é¢˜ã€‚
å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯šå®åœ°è¯´ä¸çŸ¥é“ã€‚"""
        
        prompt = f"""åŸºäºä»¥ä¸‹æ–‡æ¡£ç‰‡æ®µå›ç­”é—®é¢˜ï¼š

{context}

é—®é¢˜ï¼š{question}

è¯·åŸºäºä¸Šè¿°æ–‡æ¡£ç‰‡æ®µå›ç­”é—®é¢˜ï¼Œå¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´æ˜ã€‚"""
        
        # 7. ç”Ÿæˆå›ç­”
        answer = self.llm_client.generate(
            prompt=prompt,
            system_prompt=system_prompt,
            temperature=0.1,
        )
        
        return {
            "answer": answer,
            "sources": context_chunks,
            # å‘åå…¼å®¹ï¼šæœ‰äº›è¯„ä¼°è„šæœ¬æˆ–ä¸Šå±‚ä¼šç”¨ chunks è¡¨ç¤ºæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡å—
            "chunks": context_chunks,
            "query": question,
        }
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯"""
        return self.vector_store.get_stats(self.kb_id)
    
    def delete_knowledge_base(self):
        """åˆ é™¤æ•´ä¸ªçŸ¥è¯†åº“"""
        self.vector_store.delete_knowledge_base(self.kb_id)


__all__ = ["RAGEngine"]
