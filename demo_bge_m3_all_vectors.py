#!/usr/bin/env python3
"""
BGE-M3 All-in-One å‘é‡åŒ–æ¼”ç¤ºè„šæœ¬
-------------------------------
"""

import os
import torch

# ç»•è¿‡ transformers çš„å¼ºåˆ¶ç‰ˆæœ¬æ£€æŸ¥ (CVE-2025-32434)
# è¡¥ä¸ï¼šåœ¨æ‰€æœ‰å¯èƒ½çš„æ¨¡å—å‰¯æœ¬ä¸­åºŸæ‰è¿™ä¸ªæ£€æŸ¥å‡½æ•°
def patch_transformers_security_check():
    try:
        import transformers.utils.import_utils as iu
        iu.check_torch_load_is_safe = lambda: None
        
        import transformers.utils as u
        if hasattr(u, "check_torch_load_is_safe"):
            u.check_torch_load_is_safe = lambda: None
            
        import transformers.modeling_utils as mu
        if hasattr(mu, "check_torch_load_is_safe"):
            mu.check_torch_load_is_safe = lambda: None
    except Exception:
        pass

patch_transformers_security_check()

# è‡ªåŠ¨è®¾ç½®ç¯å¢ƒå˜é‡ä»¥ä½¿ç”¨å›½å†…é•œåƒç«™ï¼Œé¿å…è¿æ¥ Hugging Face å¤±è´¥
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

try:
    from FlagEmbedding import BGEM3FlagModel
    import numpy as np
except ImportError:
    print("âŒ è¯·å…ˆå®‰è£…ä¾èµ–: pip install FlagEmbedding numpy")
    exit(1)

def demo():
    print("â³ æ­£åœ¨æ£€æŸ¥å¹¶åŠ è½½ BGE-M3 æ¨¡å‹ (BAAI/bge-m3)...")
    
    # 1. æå‰æ‰‹åŠ¨ä¸‹è½½å¹¶è·å–æœ¬åœ°ç»å¯¹è·¯å¾„
    model_path = 'BAAI/bge-m3' # é»˜è®¤å€¼
    try:
        from huggingface_hub import snapshot_download
        print("   æ­£åœ¨é€šè¿‡é•œåƒç«™åŒæ­¥æ¨¡å‹æ ¸å¿ƒç»„ä»¶...")
        # è¿™ä¸€æ­¥ä¼šè¿”å›æ¨¡å‹åœ¨ç£ç›˜ä¸Šçš„çœŸå®å­˜å‚¨è·¯å¾„
        model_path = snapshot_download(
            repo_id='BAAI/bge-m3',
            ignore_patterns=["imgs/*", ".DS_Store", "*.pdf", "*.png"], 
            local_files_only=False,
            max_workers=1,
            resume_download=True
        )
        print(f"   âœ… æ ¸å¿ƒæ–‡ä»¶æ ¡éªŒé€šè¿‡ï¼Œæœ¬åœ°è·¯å¾„: {model_path}")
    except Exception as e:
        print(f"   âš ï¸ é¢„ä¸‹è½½æç¤º: {e}")
        print("   å°è¯•ç›´æ¥å¯åŠ¨...")

    # 2. å°†æœ¬åœ°ç»å¯¹è·¯å¾„ä¼ ç»™æ¨¡å‹ï¼Œå¼ºåˆ¶å…¶ä¸å†è¿›è¡Œè¿œç¨‹æ ¡éªŒ
    print("   ğŸš€ æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹ (æ­¤æ­¥éª¤æ¶‰åŠå¤§é‡çŸ©é˜µè¿ç®—ï¼Œè¯·ç¨å€™)...")
    model = BGEM3FlagModel(model_path, use_fp16=False) 

    sentence = "ä½ å¥½ï¼Œæˆ‘æƒ³å­¦ä¹ å¦‚ä½•åˆ¶ä½œçº¢çƒ§è‚‰ã€‚"
    print(f"\nğŸ“ åŸå§‹æ–‡æœ¬: \"{sentence}\"")
    print("-" * 50)

    # è·å–ä¸‰ç§å‘é‡
    # return_sparse=True ä¼šè¿”å› lexical_weights (è¯æ±‡æƒé‡)
    # return_colbert_vecs=True ä¼šè¿”å› token çº§åˆ«çš„å‘é‡çŸ©é˜µ
    output = model.encode(
        [sentence], 
        return_dense=True, 
        return_sparse=True, 
        return_colbert_vecs=True
    )

    # 1. å¯†é›†å‘é‡ (Dense Vector)
    dense_vec = output['dense_vecs'][0]
    print(f"ã€1. å¯†é›†å‘é‡ (Dense)ã€‘")
    print(f"   ç»´åº¦: {len(dense_vec)}")
    print(f"   å‰ 5 ä¸ªåˆ†é‡: {dense_vec[:5]}")
    print(f"   ç‰¹ç‚¹: é«˜åº¦å‹ç¼©çš„è¯­ä¹‰ï¼Œç”¨äºå¿«é€Ÿå…¨å±€æœç´¢ã€‚\n")

    # 2. ç¨€ç–å‘é‡ (Sparse Vector / Lexical Weights)
    # æ³¨æ„ï¼šBGE-M3 è¿”å›çš„æ˜¯ {token_id: weight} çš„å½¢å¼
    sparse_vec = output['lexical_weights'][0]
    print(f"ã€2. ç¨€ç–å‘é‡ (Sparse / Lexical)ã€‘")
    print(f"   éé›¶ Token æ•°é‡: {len(sparse_vec)}")
    
    # æŒ‰ç…§æƒé‡æ’åºçœ‹å‰ 5 ä¸ªå…³é”®è¯ï¼ˆToken ID å½¢å¼ï¼‰
    top_tokens = sorted(sparse_vec.items(), key=lambda x: x[1], reverse=True)[:5]
    print(f"   æƒé‡æœ€é«˜çš„ 5 ä¸ª Token ID åŠå…¶æƒé‡: {top_tokens}")
    print(f"   ç‰¹ç‚¹: ç±»ä¼¼è¯é¢‘/æƒé‡ç»Ÿè®¡ï¼Œæ•æ‰å…³é”®è¯çš„ç²¾ç¡®åŒ¹é…ã€‚\n")

    # 3. å¤šå‘é‡ (Multi-Vector / ColBERT)
    # è¿”å›çš„æ˜¯ [sequence_length, vector_dim] çš„çŸ©é˜µ
    colbert_vecs = output['colbert_vecs'][0]
    print(f"ã€3. å¤šå‘é‡ (Multi-Vector / ColBERT)ã€‘")
    print(f"   çŸ©é˜µå½¢çŠ¶: {colbert_vecs.shape} (Tokenæ•°é‡ x ç»´åº¦)")
    print(f"   è§£é‡Š: æ¯ä¸ªè¯éƒ½æœ‰ä¸€ä¸ªç‹¬ä¸€æ— äºŒçš„ 1024 ç»´ç‰¹å¾ã€‚")
    print(f"   ç‰¹ç‚¹: ç”¨äºæé«˜ç²¾åº¦çš„ç²¾æ’ (Rerank)ï¼Œè®¡ç®—é‡å’Œå­˜å‚¨é‡æœ€å¤§ã€‚\n")

    print("=" * 50)
    print("ğŸ’¡ ç»“è®ºï¼šä¸€ä¸ª BGE-M3 æ¨¡å‹é€šè¿‡ä¸€æ¬¡è®¡ç®—ï¼Œå°±æä¾›äº†ä¸‰ç§äº’è¡¥çš„æ£€ç´¢ç‰¹å¾ã€‚")

if __name__ == "__main__":
    demo()

