#!/usr/bin/env python3
"""
è°ƒè¯• RAGAS è°ƒç”¨ LLM API æ—¶çš„å‚æ•°ï¼ˆæ–¹æ³• 3ï¼šä½¿ç”¨ HTTP è¯·æ±‚æ‹¦æˆªï¼‰
é€šè¿‡æ‹¦æˆª OpenAI å®¢æˆ·ç«¯çš„ HTTP è¯·æ±‚æ¥æŸ¥çœ‹å®é™…å‘é€çš„å‚æ•°
"""
import sys
from pathlib import Path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

import json
import warnings
warnings.filterwarnings('ignore', category=DeprecationWarning)

from langchain_openai import ChatOpenAI
from ragas.llms import LangchainLLMWrapper
from ragas.metrics._answer_relevance import ResponseRelevanceInput, ResponseRelevancePrompt
from config.config import AppConfig

# å­˜å‚¨æ‰€æœ‰ HTTP è¯·æ±‚
http_requests = []

def debug_ragas_api_call_v3():
    """ä½¿ç”¨ HTTP è¯·æ±‚æ‹¦æˆªè°ƒè¯•"""
    print("=" * 60)
    print("è°ƒè¯• RAGAS è°ƒç”¨ LLM APIï¼ˆæ‹¦æˆª HTTP è¯·æ±‚ï¼‰")
    print("=" * 60)
    print()
    
    # åŠ è½½é…ç½®
    app_config = AppConfig.load()
    
    # æ–¹æ³•ï¼šæ‹¦æˆª OpenAI å®¢æˆ·ç«¯çš„ HTTP è¯·æ±‚
    # é€šè¿‡ monkey patching httpx æˆ– requests æ¥æ‹¦æˆª
    
    try:
        import httpx
        from openai import OpenAI
        
        # åˆ›å»ºåŸå§‹å®¢æˆ·ç«¯
        original_client = OpenAI(
            api_key=app_config.llm.api_key,
            base_url=app_config.llm.base_url,
        )
        
        # ä¿å­˜åŸå§‹çš„ post æ–¹æ³•
        original_post = httpx.AsyncClient.post
        
        async def debug_post(self, url, **kwargs):
            """æ‹¦æˆª HTTP POST è¯·æ±‚"""
            print("\n" + "=" * 60)
            print("ğŸ” æ‹¦æˆªåˆ° HTTP POST è¯·æ±‚")
            print("=" * 60)
            print(f"ğŸ“‹ URL: {url}")
            print(f"ğŸ“‹ kwargs keys: {list(kwargs.keys())}")
            
            # æ£€æŸ¥ data æˆ– json å‚æ•°
            if 'json' in kwargs:
                print(f"\nğŸ“‹ JSON å‚æ•°:")
                json_data = kwargs['json']
                print(f"  - ç±»å‹: {type(json_data)}")
                print(f"  - å†…å®¹: {json.dumps(json_data, ensure_ascii=False, indent=2)}")
                
                # æ£€æŸ¥ messages
                if 'messages' in json_data:
                    print(f"\nğŸ“‹ Messages ({len(json_data['messages'])} ä¸ª):")
                    for i, msg in enumerate(json_data['messages']):
                        print(f"  æ¶ˆæ¯ {i+1}:")
                        print(f"    - role: {msg.get('role')}")
                        print(f"    - content ç±»å‹: {type(msg.get('content'))}")
                        print(f"    - content å€¼: {repr(msg.get('content'))}")
                        
                        # æ£€æŸ¥ content æ˜¯å¦æœ‰é—®é¢˜
                        content = msg.get('content')
                        if content is None:
                            print(f"    âš ï¸  content æ˜¯ None!")
                        elif not isinstance(content, str):
                            print(f"    âš ï¸  content ä¸æ˜¯å­—ç¬¦ä¸²: {type(content)}")
                            if isinstance(content, list):
                                print(f"    âš ï¸  content æ˜¯åˆ—è¡¨ï¼Œé•¿åº¦: {len(content)}")
                                for j, item in enumerate(content):
                                    print(f"       é¡¹ç›® {j}: ç±»å‹={type(item)}, å€¼={repr(item)}")
                
                # ä¿å­˜è¯·æ±‚ä¿¡æ¯
                http_requests.append({
                    "url": str(url),
                    "method": "POST",
                    "json": json_data,
                })
            
            # è°ƒç”¨åŸå§‹æ–¹æ³•
            print("\nğŸ“¤ å‘é€ HTTP è¯·æ±‚...")
            try:
                response = await original_post(self, url, **kwargs)
                print("âœ… HTTP è¯·æ±‚æˆåŠŸ")
                return response
            except Exception as e:
                print(f"âŒ HTTP è¯·æ±‚å¤±è´¥: {e}")
                raise
        
        # æ›¿æ¢æ–¹æ³•ï¼ˆéœ€è¦æ‰¾åˆ°æ­£ç¡®çš„ä½ç½®ï¼‰
        # æ³¨æ„ï¼šè¿™éœ€è¦æ›´æ·±å…¥çš„äº†è§£ httpx çš„å†…éƒ¨ç»“æ„
        
    except ImportError:
        print("âš ï¸  æ— æ³•å¯¼å…¥ httpxï¼Œä½¿ç”¨å…¶ä»–æ–¹æ³•")
    
    # åˆ›å»º LangChain LLM
    print("1. åˆ›å»º LangChain LLM...")
    langchain_llm = ChatOpenAI(
        model=app_config.llm.model,
        api_key=app_config.llm.api_key,
        base_url=app_config.llm.base_url,
        temperature=0.1,
        timeout=120.0,
        max_retries=3,
    )
    print(f"   âœ… LangChain LLM åˆ›å»ºæˆåŠŸ")
    print()
    
    # åˆ›å»º RAGAS LLM Wrapper
    print("2. åˆ›å»º RAGAS LLM Wrapper...")
    ragas_llm = LangchainLLMWrapper(langchain_llm)
    print(f"   âœ… RAGAS LLM Wrapper åˆ›å»ºæˆåŠŸ")
    print()
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    print("3. åˆ›å»ºæµ‹è¯•æ•°æ®...")
    test_answer = "æ ¹æ®æ–‡æ¡£ç‰‡æ®µï¼Œå¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤ç”¨å’–å–±çƒ¹é¥ªé’èŸ¹ã€‚"
    prompt_input = ResponseRelevanceInput(response=test_answer)
    print(f"   âœ… æµ‹è¯•æ•°æ®åˆ›å»ºæˆåŠŸ")
    print()
    
    # åˆ›å»º Prompt
    print("4. åˆ›å»º Prompt...")
    prompt = ResponseRelevancePrompt()
    print(f"   âœ… Prompt åˆ›å»ºæˆåŠŸ")
    print()
    
    # æµ‹è¯•ç”Ÿæˆ
    print("5. æµ‹è¯•ç”Ÿæˆé—®é¢˜ï¼ˆstrictness=1ï¼‰...")
    print("-" * 60)
    
    import asyncio
    async def test_generate():
        try:
            result = await prompt.generate_multiple(
                llm=ragas_llm,
                data=prompt_input,
                n=1,
            )
            print(f"\nâœ… ç”ŸæˆæˆåŠŸï¼Œç»“æœæ•°é‡: {len(result)}")
            return result
        except Exception as e:
            print(f"\nâŒ ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    result = asyncio.run(test_generate())
    print()
    
    print("=" * 60)
    print("è°ƒè¯•å®Œæˆ")
    print("=" * 60)
    print()
    print("ğŸ’¡ æç¤ºï¼šè¦æŸ¥çœ‹å®é™…çš„ HTTP è¯·æ±‚ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹æ–¹æ³•ï¼š")
    print("  1. ä½¿ç”¨ mitmproxy ç­‰ HTTP ä»£ç†å·¥å…·")
    print("  2. åœ¨ LangChain ä¸­å¯ç”¨è¯¦ç»†æ—¥å¿—")
    print("  3. ä½¿ç”¨å›è°ƒå‡½æ•°ï¼ˆå¦‚ä¸Šé¢çš„æ–¹æ³• 2ï¼‰")

if __name__ == "__main__":
    debug_ragas_api_call_v3()

