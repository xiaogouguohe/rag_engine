#!/usr/bin/env python3
"""
æµ‹è¯•ä½¿ç”¨ llm_factory ç›´æ¥è®¿é—® LLM APIï¼ˆä¸ä½¿ç”¨ LangChainï¼‰
"""
import sys
from pathlib import Path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

import warnings
warnings.filterwarnings('ignore', category=DeprecationWarning)

from openai import OpenAI
from ragas.llms import llm_factory
from ragas.metrics._answer_relevance import ResponseRelevanceInput, ResponseRelevancePrompt
from config.config import AppConfig

def test_llm_factory():
    """æµ‹è¯•ä½¿ç”¨ llm_factory"""
    print("=" * 60)
    print("æµ‹è¯•ä½¿ç”¨ llm_factoryï¼ˆä¸ä½¿ç”¨ LangChainï¼‰")
    print("=" * 60)
    print()
    
    # åŠ è½½é…ç½®
    app_config = AppConfig.load()
    
    # æ–¹æ³• 1: ä½¿ç”¨ llm_factoryï¼ˆæ¨èï¼Œä¸ä½¿ç”¨ LangChainï¼‰
    print("1. ä½¿ç”¨ llm_factory åˆ›å»º LLM...")
    print("-" * 60)
    
    # åˆ›å»º OpenAI å®¢æˆ·ç«¯ï¼ˆç›´æ¥ä½¿ç”¨ OpenAI SDKï¼‰
    openai_client = OpenAI(
        api_key=app_config.llm.api_key,
        base_url=app_config.llm.base_url,
    )
    
    # ä½¿ç”¨ llm_factory åˆ›å»º RAGAS LLM
    ragas_llm = llm_factory(
        model=app_config.llm.model,
        provider="openai",
        client=openai_client,
    )
    
    print(f"   âœ… LLM åˆ›å»ºæˆåŠŸ")
    print(f"   - æ¨¡å‹: {app_config.llm.model}")
    print(f"   - ç±»å‹: {type(ragas_llm)}")
    print(f"   - ä¸ä½¿ç”¨ LangChain: âœ…")
    print()
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    print("2. åˆ›å»ºæµ‹è¯•æ•°æ®...")
    test_answer = "æ ¹æ®æ–‡æ¡£ç‰‡æ®µï¼Œå¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤ç”¨å’–å–±çƒ¹é¥ªé’èŸ¹ã€‚"
    prompt_input = ResponseRelevanceInput(response=test_answer)
    print(f"   âœ… æµ‹è¯•æ•°æ®åˆ›å»ºæˆåŠŸ")
    print()
    
    # åˆ›å»º Prompt
    print("3. åˆ›å»º Prompt...")
    prompt = ResponseRelevancePrompt()
    print(f"   âœ… Prompt åˆ›å»ºæˆåŠŸ")
    print()
    
    # æµ‹è¯•ç”Ÿæˆ
    print("4. æµ‹è¯•ç”Ÿæˆé—®é¢˜...")
    print("-" * 60)
    
    import asyncio
    async def test_generate():
        try:
            result = await prompt.generate_multiple(
                llm=ragas_llm,
                data=prompt_input,
                n=1,
            )
            print(f"\nâœ… ç”ŸæˆæˆåŠŸï¼Œç»“æœæ•°é‡: {len(result)}")
            for i, r in enumerate(result, 1):
                print(f"   ç»“æœ {i}: {r.question}")
            return result
        except Exception as e:
            print(f"\nâŒ ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    result = asyncio.run(test_generate())
    print()
    
    print("=" * 60)
    print("æµ‹è¯•å®Œæˆ")
    print("=" * 60)
    print()
    print("ğŸ’¡ æ€»ç»“:")
    print("  - âœ… å¯ä»¥ä½¿ç”¨ llm_factory ç›´æ¥è®¿é—® LLM API")
    print("  - âœ… ä¸éœ€è¦ LangChain")
    print("  - âœ… è¿™æ˜¯ RAGAS æ¨èçš„æ–¹å¼ï¼ˆLangchainLLMWrapper å·²åºŸå¼ƒï¼‰")

if __name__ == "__main__":
    test_llm_factory()

