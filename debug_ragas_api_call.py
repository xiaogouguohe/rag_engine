#!/usr/bin/env python3
"""
è°ƒè¯• RAGAS è°ƒç”¨ LLM API æ—¶çš„å‚æ•°
ä½¿ç”¨ monkey patching å’Œå›è°ƒå‡½æ•°æ¥æ‹¦æˆªå’Œè®°å½• API è°ƒç”¨
"""
import sys
from pathlib import Path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

import json
import warnings
warnings.filterwarnings('ignore', category=DeprecationWarning)

from langchain_openai import ChatOpenAI
from ragas.llms import LangchainLLMWrapper
from ragas.metrics._answer_relevance import ResponseRelevanceInput, ResponseRelevancePrompt
from config.config import AppConfig

# å­˜å‚¨æ‰€æœ‰ API è°ƒç”¨è®°å½•
api_calls = []

def debug_ragas_api_call():
    """è°ƒè¯• RAGAS è°ƒç”¨ LLM API æ—¶çš„å‚æ•°"""
    print("=" * 60)
    print("è°ƒè¯• RAGAS è°ƒç”¨ LLM API æ—¶çš„å‚æ•°")
    print("=" * 60)
    print()
    
    # åŠ è½½é…ç½®
    app_config = AppConfig.load()
    
    # åˆ›å»º LangChain LLM
    print("1. åˆ›å»º LangChain LLM...")
    langchain_llm = ChatOpenAI(
        model=app_config.llm.model,
        api_key=app_config.llm.api_key,
        base_url=app_config.llm.base_url,
        temperature=0.1,
        timeout=120.0,
        max_retries=3,
    )
    print(f"   âœ… LangChain LLM åˆ›å»ºæˆåŠŸ")
    print()
    
    # æ–¹æ³• 1: ä½¿ç”¨ monkey patching æ‹¦æˆª agenerate_prompt
    print("2. è®¾ç½® API è°ƒç”¨æ‹¦æˆª...")
    original_agenerate_prompt = langchain_llm.agenerate_prompt
    
    async def debug_agenerate_prompt(prompts, stop=None, callbacks=None, **kwargs):
        """æ‹¦æˆª agenerate_prompt è°ƒç”¨"""
        print("\n" + "=" * 60)
        print("ğŸ” æ‹¦æˆªåˆ° agenerate_prompt è°ƒç”¨")
        print("=" * 60)
        
        # è®°å½•è°ƒç”¨ä¿¡æ¯
        call_info = {
            "method": "agenerate_prompt",
            "prompts_count": len(prompts),
            "stop": stop,
            "kwargs": kwargs,
        }
        
        # è¯¦ç»†æ£€æŸ¥æ¯ä¸ª prompt
        print(f"\nğŸ“‹ å‚æ•°ä¿¡æ¯:")
        print(f"  - prompts æ•°é‡: {len(prompts)}")
        print(f"  - stop: {stop}")
        print(f"  - kwargs: {kwargs}")
        print()
        
        # æ£€æŸ¥æ¯ä¸ª prompt
        for i, prompt in enumerate(prompts):
            print(f"ğŸ“ Prompt {i+1}:")
            print(f"  - ç±»å‹: {type(prompt)}")
            
            # å¦‚æœæ˜¯ PromptValueï¼Œæ£€æŸ¥å…¶å±æ€§
            if hasattr(prompt, 'text'):
                print(f"  - text ç±»å‹: {type(prompt.text)}")
                print(f"  - text å€¼: {repr(prompt.text[:200]) if prompt.text else None}")
                print(f"  - text æ˜¯å¦ä¸º None: {prompt.text is None}")
                print(f"  - text æ˜¯å¦ä¸ºç©º: {not prompt.text if prompt.text else True}")
                
                call_info[f"prompt_{i}_text"] = prompt.text
                call_info[f"prompt_{i}_text_type"] = str(type(prompt.text))
                call_info[f"prompt_{i}_text_is_none"] = prompt.text is None
            
            # è½¬æ¢ä¸ºæ¶ˆæ¯
            try:
                messages = prompt.to_messages()
                print(f"  - to_messages() æˆåŠŸï¼Œæ¶ˆæ¯æ•°é‡: {len(messages)}")
                
                for j, msg in enumerate(messages):
                    print(f"    æ¶ˆæ¯ {j+1}:")
                    print(f"      - ç±»å‹: {type(msg)}")
                    print(f"      - content ç±»å‹: {type(msg.content)}")
                    print(f"      - content å€¼: {repr(str(msg.content)[:100]) if msg.content else None}")
                    print(f"      - content æ˜¯å¦ä¸º None: {msg.content is None}")
                    print(f"      - content æ˜¯å¦ä¸ºå­—ç¬¦ä¸²: {isinstance(msg.content, str)}")
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰é—®é¢˜
                    if msg.content is None:
                        print(f"      âš ï¸  content æ˜¯ None!")
                    elif not isinstance(msg.content, str):
                        print(f"      âš ï¸  content ä¸æ˜¯å­—ç¬¦ä¸²: {type(msg.content)}")
                        if isinstance(msg.content, list):
                            print(f"      âš ï¸  content æ˜¯åˆ—è¡¨ï¼Œé•¿åº¦: {len(msg.content)}")
                            for k, item in enumerate(msg.content):
                                print(f"         é¡¹ç›® {k}: ç±»å‹={type(item)}, å€¼={repr(item)[:50]}")
                    
                    call_info[f"prompt_{i}_message_{j}_content"] = str(msg.content) if msg.content else None
                    call_info[f"prompt_{i}_message_{j}_content_type"] = str(type(msg.content))
                    call_info[f"prompt_{i}_message_{j}_content_is_none"] = msg.content is None
                    
            except Exception as e:
                print(f"  - to_messages() å¤±è´¥: {e}")
                call_info[f"prompt_{i}_to_messages_error"] = str(e)
            
            print()
        
        # ä¿å­˜è°ƒç”¨è®°å½•
        api_calls.append(call_info)
        
        # è°ƒç”¨åŸå§‹æ–¹æ³•
        print("ğŸ“¤ è°ƒç”¨åŸå§‹ agenerate_prompt...")
        try:
            result = await original_agenerate_prompt(prompts, stop=stop, callbacks=callbacks, **kwargs)
            print("âœ… API è°ƒç”¨æˆåŠŸ")
            return result
        except Exception as e:
            print(f"âŒ API è°ƒç”¨å¤±è´¥: {e}")
            print(f"   é”™è¯¯ç±»å‹: {type(e)}")
            import traceback
            traceback.print_exc()
            raise
    
    # æ›¿æ¢æ–¹æ³•
    langchain_llm.agenerate_prompt = debug_agenerate_prompt
    print("   âœ… API è°ƒç”¨æ‹¦æˆªå·²è®¾ç½®")
    print()
    
    # åˆ›å»º RAGAS LLM Wrapper
    print("3. åˆ›å»º RAGAS LLM Wrapper...")
    ragas_llm = LangchainLLMWrapper(langchain_llm)
    print(f"   âœ… RAGAS LLM Wrapper åˆ›å»ºæˆåŠŸ")
    print()
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    print("4. åˆ›å»ºæµ‹è¯•æ•°æ®...")
    test_answer = "æ ¹æ®æ–‡æ¡£ç‰‡æ®µï¼Œå¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤ç”¨å’–å–±çƒ¹é¥ªé’èŸ¹ã€‚"
    prompt_input = ResponseRelevanceInput(response=test_answer)
    print(f"   âœ… æµ‹è¯•æ•°æ®åˆ›å»ºæˆåŠŸ")
    print(f"      ç­”æ¡ˆ: {test_answer}")
    print()
    
    # åˆ›å»º Prompt
    print("5. åˆ›å»º Prompt...")
    prompt = ResponseRelevancePrompt()
    print(f"   âœ… Prompt åˆ›å»ºæˆåŠŸ")
    print()
    
    # æµ‹è¯•ç”Ÿæˆï¼ˆåªç”Ÿæˆ 1 ä¸ªé—®é¢˜ï¼Œé¿å…å…¼å®¹æ€§é—®é¢˜ï¼‰
    print("6. æµ‹è¯•ç”Ÿæˆé—®é¢˜ï¼ˆstrictness=1ï¼‰...")
    print("-" * 60)
    
    import asyncio
    async def test_generate():
        try:
            result = await prompt.generate_multiple(
                llm=ragas_llm,
                data=prompt_input,
                n=1,  # åªç”Ÿæˆ 1 ä¸ªé—®é¢˜
            )
            print(f"\nâœ… ç”ŸæˆæˆåŠŸï¼Œç»“æœæ•°é‡: {len(result)}")
            for i, r in enumerate(result, 1):
                print(f"   ç»“æœ {i}: {r.question}")
            return result
        except Exception as e:
            print(f"\nâŒ ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    result = asyncio.run(test_generate())
    print()
    
    # ä¿å­˜è°ƒç”¨è®°å½•
    print("7. ä¿å­˜ API è°ƒç”¨è®°å½•...")
    output_file = Path("ragas_api_calls_debug.json")
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(api_calls, f, ensure_ascii=False, indent=2)
    print(f"   âœ… è°ƒç”¨è®°å½•å·²ä¿å­˜åˆ°: {output_file}")
    print()
    
    print("=" * 60)
    print("è°ƒè¯•å®Œæˆ")
    print("=" * 60)
    print()
    print("ğŸ“Š è°ƒç”¨è®°å½•æ‘˜è¦:")
    print(f"  - æ€»è°ƒç”¨æ¬¡æ•°: {len(api_calls)}")
    for i, call in enumerate(api_calls, 1):
        print(f"  - è°ƒç”¨ {i}:")
        print(f"    - prompts æ•°é‡: {call.get('prompts_count', 'N/A')}")
        for key, value in call.items():
            if key.startswith('prompt_') and 'content_is_none' in key and value:
                print(f"    - âš ï¸  {key}: {value}")

if __name__ == "__main__":
    debug_ragas_api_call()

