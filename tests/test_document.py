#!/usr/bin/env python3
"""
æµ‹è¯•è„šæœ¬ï¼šéªŒè¯æ–‡æ¡£è§£æå’Œåˆ†å—åŠŸèƒ½

ä½¿ç”¨æ–¹æ³•ï¼š
    python tests/test_document.py
"""

import sys
from pathlib import Path
import tempfile
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from document import (
    ParserFactory,
    MarkdownParser,
    TextChunker,
    MetadataEnhancer,
    DataPreparationModule,
)
from langchain_core.documents import Document


def test_txt_parser():
    """æµ‹è¯• TXT æ–‡ä»¶è§£æ"""
    print("=" * 60)
    print("æµ‹è¯• 1: TXT æ–‡ä»¶è§£æ")
    print("=" * 60)
    
    # åˆ›å»ºä¸´æ—¶æµ‹è¯•æ–‡ä»¶
    test_content = """è¿™æ˜¯ç¬¬ä¸€æ®µæ–‡æœ¬ã€‚
è¿™æ˜¯ç¬¬äºŒæ®µæ–‡æœ¬ã€‚
è¿™æ˜¯ç¬¬ä¸‰æ®µæ–‡æœ¬ï¼ŒåŒ…å«ä¸€äº›ä¸­æ–‡å†…å®¹ã€‚

è¿˜æœ‰æ›´å¤šå†…å®¹åœ¨è¿™é‡Œã€‚"""
    
    with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8') as f:
        f.write(test_content)
        temp_file = f.name
    
    try:
        # ä½¿ç”¨è§£æå™¨å·¥å‚
        result = ParserFactory.parse(temp_file)
        
        print("âœ… TXT è§£ææˆåŠŸï¼")
        print(f"   æ–‡ä»¶ç±»å‹: {result['file_type']}")
        print(f"   æ–‡ä»¶å: {result['file_name']}")
        print(f"   å†…å®¹é•¿åº¦: {len(result['content'])} å­—ç¬¦")
        print(f"   å†…å®¹é¢„è§ˆ: {result['content'][:50]}...")
        
        assert result['file_type'] == 'txt'
        assert len(result['content']) > 0
        return True
        
    except Exception as e:
        print(f"âŒ TXT è§£æå¤±è´¥: {e}")
        return False
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(temp_file):
            os.unlink(temp_file)


def test_markdown_parser():
    """æµ‹è¯• Markdown æ–‡ä»¶è§£æï¼ˆä½¿ç”¨æ–°çš„ MarkdownParserï¼‰"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 2: Markdown æ–‡ä»¶è§£æï¼ˆç»“æ„æ„ŸçŸ¥ï¼‰")
    print("=" * 60)
    
    # åˆ›å»ºä¸´æ—¶æµ‹è¯•æ–‡ä»¶ï¼ˆå‚è€ƒ C8 çš„èœè°±æ ¼å¼ï¼‰
    test_content = """# è¥¿çº¢æŸ¿è±†è…æ±¤ç¾¹çš„åšæ³•

è¥¿çº¢æŸ¿è±†è…æ±¤ç¾¹æ˜¯ä¸€é“å¾ˆæ¸…æ·¡ç¾å‘³çš„æ±¤ç¾¹

é¢„ä¼°çƒ¹é¥ªéš¾åº¦ï¼šâ˜…â˜…

## å¿…å¤‡åŸæ–™å’Œå·¥å…·

* è¥¿çº¢æŸ¿
* é¸¡è›‹
* è±†è…

## è®¡ç®—

æ¯ä»½ï¼š
* è¥¿çº¢æŸ¿ 1 ä¸ª
* é¸¡è›‹ 1 ä¸ª

## æ“ä½œ

* è¥¿çº¢æŸ¿åˆ‡æˆå°ä¸
* èµ·é”…çƒ§æ²¹
"""
    
    with tempfile.NamedTemporaryFile(mode='w', suffix='.md', delete=False, encoding='utf-8') as f:
        f.write(test_content)
        temp_file = f.name
    
    try:
        parser = MarkdownParser()
        parent_doc, child_docs = parser.parse_to_documents(temp_file)
        
        print("âœ… Markdown è§£ææˆåŠŸï¼")
        print(f"   çˆ¶æ–‡æ¡£ ID: {parent_doc.metadata.get('parent_id')}")
        print(f"   å­æ–‡æ¡£æ•°é‡: {len(child_docs)}")
        print(f"   æ–‡ä»¶ç±»å‹: {parent_doc.metadata.get('file_type')}")
        
        # éªŒè¯å­æ–‡æ¡£
        for i, child in enumerate(child_docs, 1):
            print(f"\n   å­æ–‡æ¡£ {i}:")
            print(f"     æ ‡é¢˜ä¿¡æ¯: {child.metadata.get('ä¸»æ ‡é¢˜', child.metadata.get('äºŒçº§æ ‡é¢˜', 'æ— '))}")
            print(f"     å†…å®¹é¢„è§ˆ: {child.page_content[:50]}...")
            assert child.metadata.get('parent_id') == parent_doc.metadata.get('parent_id')
        
        assert len(child_docs) > 0
        return True
        
    except Exception as e:
        print(f"âŒ Markdown è§£æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    finally:
        if os.path.exists(temp_file):
            os.unlink(temp_file)


def test_text_chunker():
    """æµ‹è¯•æ–‡æœ¬åˆ†å—ï¼ˆå›ºå®šå¤§å°ï¼‰"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 3: æ–‡æœ¬åˆ†å—ï¼ˆå›ºå®šå¤§å°ï¼‰")
    print("=" * 60)
    
    # åˆ›å»ºæµ‹è¯•æ–‡æœ¬ï¼ˆè¾ƒé•¿ï¼‰
    test_text = """è¿™æ˜¯ç¬¬ä¸€æ®µæ–‡æœ¬ã€‚åŒ…å«ä¸€äº›å†…å®¹ã€‚
è¿™æ˜¯ç¬¬äºŒæ®µæ–‡æœ¬ã€‚ä¹ŸåŒ…å«ä¸€äº›å†…å®¹ã€‚
è¿™æ˜¯ç¬¬ä¸‰æ®µæ–‡æœ¬ã€‚ç»§ç»­åŒ…å«å†…å®¹ã€‚
è¿™æ˜¯ç¬¬å››æ®µæ–‡æœ¬ã€‚è¿˜æœ‰æ›´å¤šå†…å®¹ã€‚
è¿™æ˜¯ç¬¬äº”æ®µæ–‡æœ¬ã€‚æœ€åä¸€æ®µå†…å®¹ã€‚"""
    
    chunker = TextChunker(
        chunk_size=50,  # æ¯å— 50 å­—ç¬¦
        chunk_overlap=10,  # é‡å  10 å­—ç¬¦
        use_markdown_header_split=False,  # ä¸ä½¿ç”¨æ ‡é¢˜åˆ†å‰²
    )
    
    try:
        chunks = chunker.split_text(test_text)
        
        print("âœ… æ–‡æœ¬åˆ†å—æˆåŠŸï¼")
        print(f"   åŸå§‹æ–‡æœ¬é•¿åº¦: {len(test_text)} å­—ç¬¦")
        print(f"   åˆ†å—æ•°é‡: {len(chunks)}")
        print(f"   æ¯å—å¤§å°: {chunker.chunk_size} å­—ç¬¦")
        print(f"   é‡å å¤§å°: {chunker.chunk_overlap} å­—ç¬¦")
        
        for i, chunk in enumerate(chunks, 1):
            print(f"\n   å— {i}:")
            print(f"     æ–‡æœ¬: {chunk['text'][:40]}...")
            if 'start' in chunk:
                print(f"     ä½ç½®: {chunk['start']}-{chunk['end']}")
            print(f"     é•¿åº¦: {len(chunk['text'])} å­—ç¬¦")
        
        assert len(chunks) > 0
        assert all(len(chunk['text']) > 0 for chunk in chunks)
        return True
        
    except Exception as e:
        print(f"âŒ æ–‡æœ¬åˆ†å—å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_markdown_header_chunker():
    """æµ‹è¯• Markdown æ ‡é¢˜åˆ†å‰²ï¼ˆå‚è€ƒ C8ï¼‰"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 3.5: Markdown æ ‡é¢˜åˆ†å‰²ï¼ˆå‚è€ƒ C8ï¼‰")
    print("=" * 60)
    
    test_content = """# è¥¿çº¢æŸ¿è±†è…æ±¤ç¾¹çš„åšæ³•

è¿™æ˜¯ä¸€é“å¾ˆæ¸…æ·¡ç¾å‘³çš„æ±¤ç¾¹

é¢„ä¼°çƒ¹é¥ªéš¾åº¦ï¼šâ˜…â˜…

## å¿…å¤‡åŸæ–™å’Œå·¥å…·

* è¥¿çº¢æŸ¿
* é¸¡è›‹
* è±†è…

## è®¡ç®—

æ¯ä»½ï¼š
* è¥¿çº¢æŸ¿ 1 ä¸ª
* é¸¡è›‹ 1 ä¸ª

## æ“ä½œ

* è¥¿çº¢æŸ¿åˆ‡æˆå°ä¸
* èµ·é”…çƒ§æ²¹
"""
    
    chunker = TextChunker(
        use_markdown_header_split=True,  # ä½¿ç”¨æ ‡é¢˜åˆ†å‰²
    )
    
    try:
        chunks = chunker.split_text(test_content, file_type="markdown")
        
        print("âœ… Markdown æ ‡é¢˜åˆ†å‰²æˆåŠŸï¼")
        print(f"   åŸå§‹æ–‡æœ¬é•¿åº¦: {len(test_content)} å­—ç¬¦")
        print(f"   åˆ†å—æ•°é‡: {len(chunks)}")
        
        for i, chunk in enumerate(chunks, 1):
            print(f"\n   å— {i}:")
            print(f"     æ–‡æœ¬: {chunk['text'][:50]}...")
            if 'metadata' in chunk:
                metadata = chunk['metadata']
                if 'ä¸»æ ‡é¢˜' in metadata:
                    print(f"     ä¸»æ ‡é¢˜: {metadata['ä¸»æ ‡é¢˜']}")
                if 'äºŒçº§æ ‡é¢˜' in metadata:
                    print(f"     äºŒçº§æ ‡é¢˜: {metadata['äºŒçº§æ ‡é¢˜']}")
        
        assert len(chunks) > 0
        return True
        
    except Exception as e:
        print(f"âŒ Markdown æ ‡é¢˜åˆ†å‰²å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_chunker_with_metadata():
    """æµ‹è¯•å¸¦å…ƒæ•°æ®çš„åˆ†å—"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 4: å¸¦å…ƒæ•°æ®çš„åˆ†å—")
    print("=" * 60)
    
    test_text = """è¿™æ˜¯ç¬¬ä¸€æ®µæ–‡æœ¬ã€‚åŒ…å«ä¸€äº›å†…å®¹ã€‚
è¿™æ˜¯ç¬¬äºŒæ®µæ–‡æœ¬ã€‚ä¹ŸåŒ…å«ä¸€äº›å†…å®¹ã€‚
è¿™æ˜¯ç¬¬ä¸‰æ®µæ–‡æœ¬ã€‚ç»§ç»­åŒ…å«å†…å®¹ã€‚"""
    
    metadata = {
        "doc_id": "test_doc_001",
        "file_name": "test.txt",
        "file_type": "txt",
    }
    
    chunker = TextChunker(chunk_size=40, chunk_overlap=5)
    
    try:
        chunks = chunker.chunk_document(test_text, metadata=metadata)
        
        print("âœ… å¸¦å…ƒæ•°æ®çš„åˆ†å—æˆåŠŸï¼")
        print(f"   åˆ†å—æ•°é‡: {len(chunks)}")
        
        for i, chunk in enumerate(chunks, 1):
            print(f"\n   å— {i}:")
            print(f"     æ–‡æœ¬: {chunk['text'][:30]}...")
            print(f"     å…ƒæ•°æ®: doc_id={chunk['metadata']['doc_id']}, chunk_index={chunk['metadata']['chunk_index']}")
        
        assert len(chunks) > 0
        assert all('metadata' in chunk for chunk in chunks)
        assert all(chunk['metadata']['doc_id'] == 'test_doc_001' for chunk in chunks)
        return True
        
    except Exception as e:
        print(f"âŒ å¸¦å…ƒæ•°æ®çš„åˆ†å—å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_metadata_enhancer():
    """æµ‹è¯•å…ƒæ•°æ®å¢å¼ºï¼ˆå‚è€ƒ C8ï¼‰"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 4.5: å…ƒæ•°æ®å¢å¼ºï¼ˆå‚è€ƒ C8ï¼‰")
    print("=" * 60)
    
    # åˆ›å»ºæµ‹è¯•æ–‡ä»¶ï¼ˆåŒ…å«éš¾åº¦ä¿¡æ¯ï¼‰
    test_content = """# è¥¿çº¢æŸ¿è±†è…æ±¤ç¾¹çš„åšæ³•

é¢„ä¼°çƒ¹é¥ªéš¾åº¦ï¼šâ˜…â˜…

è¿™æ˜¯ä¸€é“å¾ˆæ¸…æ·¡ç¾å‘³çš„æ±¤ç¾¹
"""
    
    with tempfile.NamedTemporaryFile(mode='w', suffix='.md', delete=False, encoding='utf-8') as f:
        f.write(test_content)
        temp_file = f.name
    
    try:
        enhancer = MetadataEnhancer()
        metadata = {
            "file_name": "test.md",
            "file_type": "markdown",
        }
        
        enhanced = enhancer.enhance(metadata, Path(temp_file), test_content)
        
        print("âœ… å…ƒæ•°æ®å¢å¼ºæˆåŠŸï¼")
        print(f"   åŸå§‹å…ƒæ•°æ®: {metadata}")
        print(f"   å¢å¼ºåå…ƒæ•°æ®: {enhanced}")
        
        # éªŒè¯éš¾åº¦æå–
        assert "difficulty" in enhanced
        assert enhanced["difficulty"] == "ç®€å•"  # â˜…â˜… = ç®€å•
        
        return True
        
    except Exception as e:
        print(f"âŒ å…ƒæ•°æ®å¢å¼ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    finally:
        if os.path.exists(temp_file):
            os.unlink(temp_file)


def test_data_preparation_module():
    """æµ‹è¯•æ•°æ®å‡†å¤‡æ¨¡å—ï¼ˆå‚è€ƒ C8ï¼‰"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 5: æ•°æ®å‡†å¤‡æ¨¡å—ï¼ˆå‚è€ƒ C8ï¼‰")
    print("=" * 60)
    
    # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
    test_content = """# è¥¿çº¢æŸ¿è±†è…æ±¤ç¾¹çš„åšæ³•

é¢„ä¼°çƒ¹é¥ªéš¾åº¦ï¼šâ˜…â˜…

## å¿…å¤‡åŸæ–™å’Œå·¥å…·

* è¥¿çº¢æŸ¿
* é¸¡è›‹

## æ“ä½œ

* è¥¿çº¢æŸ¿åˆ‡æˆå°ä¸
* èµ·é”…çƒ§æ²¹
"""
    
    with tempfile.NamedTemporaryFile(mode='w', suffix='.md', delete=False, encoding='utf-8') as f:
        f.write(test_content)
        temp_file = f.name
    
    try:
        # ä½¿ç”¨æ•°æ®å‡†å¤‡æ¨¡å—
        data_module = DataPreparationModule(use_markdown_header_split=True)
        
        # åŠ è½½æ–‡æ¡£
        parent_docs = data_module.load_documents([temp_file])
        
        print("âœ… æ•°æ®å‡†å¤‡æ¨¡å—æµ‹è¯•æˆåŠŸï¼")
        print(f"   çˆ¶æ–‡æ¡£æ•°é‡: {len(parent_docs)}")
        print(f"   å­æ–‡æ¡£æ•°é‡: {len(data_module.chunks)}")
        
        # éªŒè¯çˆ¶å­å…³ç³»
        if parent_docs:
            parent_doc = parent_docs[0]
            print(f"   çˆ¶æ–‡æ¡£ ID: {parent_doc.metadata.get('parent_id')}")
            print(f"   çˆ¶æ–‡æ¡£å…ƒæ•°æ®: {parent_doc.metadata.get('difficulty', 'æœªçŸ¥')}")
            
            # éªŒè¯å­æ–‡æ¡£
            child_count = sum(
                1 for chunk in data_module.chunks
                if chunk.metadata.get('parent_id') == parent_doc.metadata.get('parent_id')
            )
            print(f"   è¯¥æ–‡æ¡£çš„å­å—æ•°é‡: {child_count}")
        
        # æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯
        stats = data_module.get_statistics()
        print(f"   ç»Ÿè®¡ä¿¡æ¯: {stats}")
        
        assert len(parent_docs) > 0
        return True
        
    except Exception as e:
        print(f"âŒ æ•°æ®å‡†å¤‡æ¨¡å—æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    finally:
        if os.path.exists(temp_file):
            os.unlink(temp_file)


def test_integration():
    """æµ‹è¯•å®Œæ•´æµç¨‹ï¼šè§£æ + åˆ†å—"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 6: å®Œæ•´æµç¨‹ï¼ˆè§£æ + åˆ†å—ï¼‰")
    print("=" * 60)
    
    # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
    test_content = """è¿™æ˜¯æ–‡æ¡£çš„ç¬¬ä¸€æ®µå†…å®¹ã€‚åŒ…å«ä¸€äº›é‡è¦ä¿¡æ¯ã€‚
è¿™æ˜¯æ–‡æ¡£çš„ç¬¬äºŒæ®µå†…å®¹ã€‚ç»§ç»­æä¾›æ›´å¤šä¿¡æ¯ã€‚
è¿™æ˜¯æ–‡æ¡£çš„ç¬¬ä¸‰æ®µå†…å®¹ã€‚æœ€åä¸€æ®µå†…å®¹ã€‚"""
    
    with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8') as f:
        f.write(test_content)
        temp_file = f.name
    
    try:
        # 1. ä½¿ç”¨è§£æå™¨å·¥å‚è§£æ
        result = ParserFactory.parse(temp_file)
        
        # 2. åˆ†å—
        chunker = TextChunker(chunk_size=30, chunk_overlap=5)
        chunks = chunker.chunk_document(
            result['content'],
            metadata={
                "doc_id": "test_doc_002",
                "file_name": result['file_name'],
                "file_type": result['file_type'],
            }
        )
        
        print("âœ… å®Œæ•´æµç¨‹æµ‹è¯•æˆåŠŸï¼")
        print(f"   æ–‡æ¡£: {result['file_name']}")
        print(f"   åŸå§‹å†…å®¹é•¿åº¦: {len(result['content'])} å­—ç¬¦")
        print(f"   åˆ†å—æ•°é‡: {len(chunks)}")
        
        for i, chunk in enumerate(chunks, 1):
            print(f"\n   å— {i}:")
            print(f"     æ–‡æœ¬: {chunk['text']}")
            print(f"     å…ƒæ•°æ®: {chunk['metadata']}")
        
        assert len(chunks) > 0
        return True
        
    except Exception as e:
        print(f"âŒ å®Œæ•´æµç¨‹æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    finally:
        if os.path.exists(temp_file):
            os.unlink(temp_file)


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("\nğŸš€ RAG å¼•æ“ - æ–‡æ¡£è§£æå’Œåˆ†å—æµ‹è¯•\n")
    
    results = []
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    results.append(("TXT è§£æ", test_txt_parser()))
    results.append(("Markdown è§£æï¼ˆç»“æ„æ„ŸçŸ¥ï¼‰", test_markdown_parser()))
    results.append(("æ–‡æœ¬åˆ†å—ï¼ˆå›ºå®šå¤§å°ï¼‰", test_text_chunker()))
    results.append(("Markdown æ ‡é¢˜åˆ†å‰²", test_markdown_header_chunker()))
    results.append(("å…ƒæ•°æ®å¢å¼º", test_metadata_enhancer()))
    results.append(("æ•°æ®å‡†å¤‡æ¨¡å—", test_data_preparation_module()))
    results.append(("å¸¦å…ƒæ•°æ®çš„åˆ†å—", test_chunker_with_metadata()))
    results.append(("å®Œæ•´æµç¨‹", test_integration()))
    
    # æ€»ç»“
    print("\n" + "=" * 60)
    print("æµ‹è¯•æ€»ç»“")
    print("=" * 60)
    
    for name, result in results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{name}: {status}")
    
    all_passed = all(result for _, result in results)
    
    if all_passed:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ–‡æ¡£è§£æå’Œåˆ†å—åŠŸèƒ½æ­£å¸¸ã€‚")
        return 0
    else:
        print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯ã€‚")
        return 1


if __name__ == "__main__":
    sys.exit(main())
