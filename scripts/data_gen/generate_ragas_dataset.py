#!/usr/bin/env python3
"""
ä½¿ç”¨ RAGAS ç”Ÿæˆ RAG æµ‹è¯•é›†
-------------------------

RAGAS (Retrieval-Augmented Generation Assessment) æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼° RAG ç³»ç»Ÿçš„æ¡†æ¶ã€‚
æœ¬è„šæœ¬ä¸“æ³¨äºä½¿ç”¨ RAGAS çš„ TestsetGenerator ä»ã€æŒ‡å®šè·¯å¾„ã€‘çš„æ–‡æ¡£è‡ªåŠ¨ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ã€‚

åŠŸèƒ½ï¼š
1. ä»æŒ‡å®šçš„æœ¬åœ°ç›®å½•è¯»å–æ–‡æ¡£ï¼ˆå¿…é¡»æ‰‹åŠ¨æŒ‡å®š --source-pathï¼‰
2. ä½¿ç”¨ LLM æå–å®ä½“ã€å…³é”®è¯å¹¶æ„å»ºå†…éƒ¨çŸ¥è¯†å›¾è°±ï¼ˆå¯é€‰ï¼‰
3. åˆæˆåŒ…å«é—®é¢˜ã€å‚è€ƒç­”æ¡ˆå’Œå‚è€ƒä¸Šä¸‹æ–‡çš„æµ‹è¯•é›†

ä½¿ç”¨æ–¹æ³•ï¼š
    # ä½¿ç”¨æ¨èçš„æ–°æ–¹æ³•ç”Ÿæˆæµ‹è¯•é›†ï¼ˆå¿…é¡»æŒ‡å®š --source-pathï¼‰
    python scripts/data_gen/generate_ragas_dataset.py --kb-id recipes_kb --source-path ./sample_recipes --use-testset-generator

    # ä½¿ç”¨çŸ¥è¯†å›¾è°±ç”Ÿæˆ
    python scripts/data_gen/generate_ragas_dataset.py --kb-id recipes_kb --source-path ./sample_recipes --use-testset-generator --use-kg
"""

import sys
import argparse
import json
from pathlib import Path
from typing import List, Dict, Any, Optional

# å¯¼å…¥ openai ä»¥å¤„ç†è¿æ¥é”™è¯¯
try:
    import openai
except ImportError:
    openai = None

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from config import AppConfig
from document import ParserFactory, DataPreparationModule
from embedding import EmbeddingClient

# --- è¡¥ä¸ï¼šç»•è¿‡ transformers çš„å¼ºåˆ¶ç‰ˆæœ¬æ£€æŸ¥ (CVE-2025-32434) ---
def patch_transformers_security_check():
    try:
        import transformers.utils.import_utils as iu
        iu.check_torch_load_is_safe = lambda: None
        import transformers.utils as u
        if hasattr(u, "check_torch_load_is_safe"):
            u.check_torch_load_is_safe = lambda: None
        import transformers.modeling_utils as mu
        if hasattr(mu, "check_torch_load_is_safe"):
            mu.check_torch_load_is_safe = lambda: None
    except Exception:
        pass

patch_transformers_security_check()

class RagasLocalBGEAdapter:
    """é€‚é…å™¨ï¼šå°†æœ¬åœ° EmbeddingClient è½¬æ¢ä¸º RAGAS å…¼å®¹æ ¼å¼"""
    def __init__(self, embedding_client: EmbeddingClient):
        self.client = embedding_client
    
    def embed_text(self, text: str) -> List[float]:
        res = self.client.embed_texts([text])
        return res["dense_vecs"][0]
    
    def embed_texts(self, texts: List[str]) -> List[List[float]]:
        res = self.client.embed_texts(texts)
        return res["dense_vecs"]
    
    def embed_query(self, text: str) -> List[float]:
        return self.embed_text(text)
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        return self.embed_texts(texts)

# å°è¯•å¯¼å…¥ RAGAS TestsetGenerator
try:
    from ragas.testset import TestsetGenerator
    from datasets import Dataset
    from langchain_core.documents import Document as LangchainDocument
    _HAS_TESTSET_GENERATOR = True
except ImportError as e:
    TestsetGenerator = None
    _HAS_TESTSET_GENERATOR = False
    print(f"âš ï¸  TestsetGenerator ä¸å¯ç”¨: {e}")

def find_markdown_files(directory: Path) -> List[Path]:
    """é€’å½’æŸ¥æ‰¾æ‰€æœ‰ .md æ–‡ä»¶"""
    return DataPreparationModule.find_files(directory, "*.md")


def convert_documents_to_langchain_docs(file_paths: List[Path]) -> List[LangchainDocument]:
    """å°†æœ¬åœ°æ–‡æ¡£æ–‡ä»¶è½¬æ¢ä¸º langchain Document æ ¼å¼"""
    langchain_docs = []
    for file_path in file_paths:
        try:
            parser = ParserFactory.get_parser(file_path)
            if parser:
                result = parser.parse(file_path)
                content = result.get("content", "")
                if not isinstance(content, str):
                    content = str(content) if content is not None else ""
                if not content.strip():
                    continue
                langchain_docs.append(LangchainDocument(
                    page_content=content,
                    metadata={"source": str(file_path), "file_name": file_path.name}
                ))
        except Exception:
            continue
    return langchain_docs


def generate_ragas_dataset_with_knowledge_graph(
    kb_id: str,
    source_path: str,
    output_path: str = "ragas_dataset.json",
    max_docs: int = 5,
    num_questions_per_doc: int = 3,
    use_kg: bool = True,
) -> bool:
    """ä½¿ç”¨ RAGAS TestsetGenerator ä»æŒ‡å®šè·¯å¾„ç”Ÿæˆæµ‹è¯•é›†"""
    if not _HAS_TESTSET_GENERATOR:
        print("âŒ RAGAS TestsetGenerator ä¸å¯ç”¨")
        return False
    
    source_dir = Path(source_path)
    if not source_dir.exists() or not source_dir.is_dir():
        print(f"âŒ æŒ‡å®šçš„æ–‡æ¡£è·¯å¾„ä¸å­˜åœ¨æˆ–ä¸æ˜¯ç›®å½•: {source_path}")
        return False
    
    print("=" * 60)
    print(f"ä»è·¯å¾„ [{source_path}] ç”Ÿæˆæµ‹è¯•é›† (KG: {use_kg})")
    print("=" * 60)
    
    # 1. è¯»å–æœ¬åœ°æ–‡ä»¶
    md_files = find_markdown_files(source_dir)
    if max_docs:
        md_files = md_files[:max_docs]
    if not md_files:
        print(f"âŒ åœ¨è·¯å¾„ {source_path} ä¸‹æœªæ‰¾åˆ°ä»»ä½• .md æ–‡ä»¶")
        return False
    
    langchain_docs = convert_documents_to_langchain_docs(md_files)
    if len(langchain_docs) < 3:
        print(f"âš ï¸  è­¦å‘Š: æ–‡æ¡£æ•°é‡å°‘äº 3 ä¸ª ({len(langchain_docs)})ï¼ŒRAGAS ç”Ÿæˆå¯èƒ½ä¼šæŠ¥é”™")
    
    # 2. åˆå§‹åŒ–æ¨¡å‹
    try:
        app_config = AppConfig.load()
        from openai import OpenAI
        from ragas.llms import llm_factory
        
        __import__("os").environ["OPENAI_API_KEY"] = app_config.llm.api_key
        generator_llm = llm_factory(model=app_config.llm.model, base_url=app_config.llm.base_url)
        
        if app_config.embedding.mode == "local":
            embeddings = RagasLocalBGEAdapter(EmbeddingClient.from_config(app_config))
        else:
            from ragas.embeddings import OpenAIEmbeddings as RagasOpenAIEmbeddings
            embeddings = RagasOpenAIEmbeddings(
                client=OpenAI(api_key=app_config.embedding.api_key, base_url=app_config.embedding.base_url),
                model=app_config.embedding.model
            )
        
        # 3. æ„å»ºçŸ¥è¯†å›¾è°± (å¯é€‰)
        knowledge_graph = None
        if use_kg:
            print("æ­£åœ¨æ„å»ºçŸ¥è¯†å›¾è°±...")
            try:
                from ragas.testset.graph import KnowledgeGraph, Node, NodeType
                from ragas.testset.transforms.extractors import NERExtractor, KeyphrasesExtractor
                from ragas.testset.transforms import Parallel, apply_transforms
                from ragas.testset.transforms.base import BaseGraphTransformation
                from dataclasses import dataclass
                import typing as t
                
                nodes = [Node(properties={"page_content": d.page_content}, type=NodeType.DOCUMENT) for d in langchain_docs]
                kg = KnowledgeGraph(nodes=nodes)
                
                @dataclass
                class EntityFormatFixer(BaseGraphTransformation):
                    async def transform(self, kg: KnowledgeGraph) -> t.Any:
                        all_entities = set()
                        for node in kg.nodes:
                            entities = node.properties.get("entities", [])
                            flat = [str(x) for x in entities] if isinstance(entities, list) else []
                            if isinstance(entities, dict):
                                for v in entities.values():
                                    if isinstance(v, list): flat.extend([str(x) for x in v])
                            flat = list(set(flat))
                            all_entities.update(flat)
                            node.properties["entities_dict"] = {"all": flat}
                            node.properties["entities"] = flat
                        kg.properties["themes"] = list(all_entities)
                        return kg
                    def generate_execution_plan(self, kg: KnowledgeGraph):
                        async def run(): await self.transform(kg)
                        return [run()]

                from ragas.testset.transforms.relationship_builders.traditional import JaccardSimilarityBuilder
                transforms = [
                    Parallel(NERExtractor(llm=generator_llm), KeyphrasesExtractor(llm=generator_llm)),
                    EntityFormatFixer(),
                    JaccardSimilarityBuilder(property_name="entities_dict", key_name="all", threshold=0.01)
                ]
                apply_transforms(kg, transforms)
                knowledge_graph = kg
                print(f"  âœ… çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ (èŠ‚ç‚¹: {len(kg.nodes)}, è¾¹: {len(kg.relationships)})")
            except Exception as e:
                print(f"âš ï¸  å›¾è°±æ„å»ºå¤±è´¥ï¼Œå›é€€åˆ°æ™®é€šæ¨¡å¼: {e}")

        # 4. ç”Ÿæˆæµ‹è¯•é›†
        generator = TestsetGenerator(llm=generator_llm, embedding_model=embeddings, knowledge_graph=knowledge_graph)
        total_q = max_docs * num_questions_per_doc
        print(f"å¼€å§‹ç”Ÿæˆæµ‹è¯•é›† (ç›®æ ‡æ•°é‡: {total_q})...")
        
        if knowledge_graph:
            from ragas.testset.persona import Persona
            from ragas.run_config import RunConfig
            generator.persona_list = [
                Persona(name="åˆçº§ç”¨æˆ·", role_description="å¯¹é¢†åŸŸä¸ç†Ÿæ‚‰ï¼Œå€¾å‘äºé—®åŸºç¡€æ“ä½œå’Œæ ¸å¿ƒæ¦‚å¿µã€‚"),
                Persona(name="é«˜çº§ä¸“å®¶", role_description="å¯¹ç»†èŠ‚éå¸¸æ•æ„Ÿï¼Œå€¾å‘äºé—®æ·±å±‚é€»è¾‘å’Œå¤šæ–‡æ¡£å¯¹æ¯”ã€‚")
            ]
            testset = generator.generate(testset_size=total_q, run_config=RunConfig(max_workers=3, timeout=120))
        else:
            testset = generator.generate_with_langchain_docs(
                documents=langchain_docs, testset_size=total_q,
                transforms_embedding_model=embeddings, raise_exceptions=False
            )
        
        if not testset:
            print("âŒ ç”Ÿæˆç»“æœä¸ºç©º")
            return False
            
        # 5. ä¿å­˜ç»“æœ
        testset_df = testset.to_pandas()
        samples = []
        for _, row in testset_df.iterrows():
            samples.append({
                "question": row.get("user_input", row.get("question", "")),
                "answer": "",
                "ground_truth": row.get("reference", row.get("ground_truth", "")),
                "contexts": row.get("reference_contexts", row.get("contexts", [])),
                "metadata": {"kb_id": kb_id}
            })
            
        output_data = {
            "metadata": {"kb_id": kb_id, "source_path": source_path, "total_samples": len(samples)},
            "samples": samples
        }
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ç”ŸæˆæˆåŠŸ! æ–‡ä»¶ä¿å­˜è‡³: {output_path}")
        return True
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå‡ºé”™: {e}")
        __import__("traceback").print_exc()
        return False


def main():
    parser = argparse.ArgumentParser(description="ä»æŒ‡å®šè·¯å¾„ç”Ÿæˆ RAGAS æµ‹è¯•é›†")
    parser.add_argument("--kb-id", required=True, help="çŸ¥è¯†åº“ ID")
    parser.add_argument("--source-path", required=True, help="ã€å¿…å¡«ã€‘å¾…å¤„ç†æ–‡æ¡£çš„æœ¬åœ°ç›®å½•è·¯å¾„")
    parser.add_argument("--output", default="ragas_dataset.json", help="è¾“å‡º JSON æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--max-docs", type=int, default=5, help="æœ€å¤šå¤„ç†çš„æ–‡æ¡£æ•°")
    parser.add_argument("--max-questions-per-doc", type=int, default=3, help="æ¯ä¸ªæ–‡æ¡£ç”Ÿæˆçš„é—®é¢˜æ•°")
    parser.add_argument("--use-testset-generator", action="store_true", help="ä½¿ç”¨ RAGAS TestsetGenerator (æ¨è)")
    parser.add_argument("--use-kg", action="store_true", help="ä½¿ç”¨çŸ¥è¯†å›¾è°±æ¨¡å¼ (éœ€å¼€å¯ --use-testset-generator)")
    
    args = parser.parse_args()
    
    # å¼ºåˆ¶è¦æ±‚å¿…é¡»æ˜¯ TestsetGenerator æ¨¡å¼ï¼ˆæ—¢ç„¶æ˜¯ä¸ºæ‚¨å®šåˆ¶çš„é€»è¾‘ï¼Œç®€åŒ–åˆ†æ”¯ï¼‰
    if not args.use_testset_generator:
        print("ğŸ’¡ æç¤º: å»ºè®®å¼€å¯ --use-testset-generator ä»¥è·å¾—æ›´å¥½çš„ç”Ÿæˆæ•ˆæœ")
    
    success = generate_ragas_dataset_with_knowledge_graph(
        kb_id=args.kb_id,
        source_path=args.source_path,
        output_path=args.output,
        max_docs=args.max_docs,
        num_questions_per_doc=args.max_questions_per_doc,
        use_kg=args.use_kg
    )
    
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
