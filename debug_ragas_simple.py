#!/usr/bin/env python3
"""
æœ€ç®€å•çš„è°ƒè¯•æ–¹æ³•ï¼šå¯ç”¨ LangChain çš„è¯¦ç»†æ—¥å¿—
"""
import sys
from pathlib import Path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

import logging
import warnings
warnings.filterwarnings('ignore', category=DeprecationWarning)

# å¯ç”¨è¯¦ç»†æ—¥å¿—
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# å¯ç”¨ LangChain å’Œ OpenAI çš„æ—¥å¿—
logging.getLogger('langchain').setLevel(logging.DEBUG)
logging.getLogger('openai').setLevel(logging.DEBUG)
logging.getLogger('httpx').setLevel(logging.DEBUG)

from langchain_openai import ChatOpenAI
from ragas.llms import LangchainLLMWrapper
from ragas.metrics._answer_relevance import ResponseRelevanceInput, ResponseRelevancePrompt
from config.config import AppConfig

def debug_with_logging():
    """ä½¿ç”¨æ—¥å¿—è°ƒè¯•"""
    print("=" * 60)
    print("è°ƒè¯• RAGAS è°ƒç”¨ LLM APIï¼ˆä½¿ç”¨è¯¦ç»†æ—¥å¿—ï¼‰")
    print("=" * 60)
    print()
    print("ğŸ’¡ æ‰€æœ‰ API è°ƒç”¨éƒ½ä¼šæ˜¾ç¤ºåœ¨æ—¥å¿—ä¸­")
    print()
    
    # åŠ è½½é…ç½®
    app_config = AppConfig.load()
    
    # åˆ›å»º LangChain LLM
    print("1. åˆ›å»º LangChain LLM...")
    langchain_llm = ChatOpenAI(
        model=app_config.llm.model,
        api_key=app_config.llm.api_key,
        base_url=app_config.llm.base_url,
        temperature=0.1,
        timeout=120.0,
        max_retries=3,
    )
    print(f"   âœ… LangChain LLM åˆ›å»ºæˆåŠŸ")
    print()
    
    # åˆ›å»º RAGAS LLM Wrapper
    print("2. åˆ›å»º RAGAS LLM Wrapper...")
    ragas_llm = LangchainLLMWrapper(langchain_llm)
    print(f"   âœ… RAGAS LLM Wrapper åˆ›å»ºæˆåŠŸ")
    print()
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    print("3. åˆ›å»ºæµ‹è¯•æ•°æ®...")
    test_answer = "æ ¹æ®æ–‡æ¡£ç‰‡æ®µï¼Œå¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤ç”¨å’–å–±çƒ¹é¥ªé’èŸ¹ã€‚"
    prompt_input = ResponseRelevanceInput(response=test_answer)
    print(f"   âœ… æµ‹è¯•æ•°æ®åˆ›å»ºæˆåŠŸ")
    print()
    
    # åˆ›å»º Prompt
    print("4. åˆ›å»º Prompt...")
    prompt = ResponseRelevancePrompt()
    print(f"   âœ… Prompt åˆ›å»ºæˆåŠŸ")
    print()
    
    # æµ‹è¯•ç”Ÿæˆ
    print("5. æµ‹è¯•ç”Ÿæˆé—®é¢˜ï¼ˆstrictness=1ï¼‰...")
    print("-" * 60)
    print("ğŸ“‹ ä¸‹é¢çš„æ—¥å¿—ä¼šæ˜¾ç¤ºæ‰€æœ‰ API è°ƒç”¨çš„è¯¦ç»†ä¿¡æ¯")
    print()
    
    import asyncio
    async def test_generate():
        try:
            result = await prompt.generate_multiple(
                llm=ragas_llm,
                data=prompt_input,
                n=1,
            )
            print(f"\nâœ… ç”ŸæˆæˆåŠŸï¼Œç»“æœæ•°é‡: {len(result)}")
            return result
        except Exception as e:
            print(f"\nâŒ ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    result = asyncio.run(test_generate())
    print()
    
    print("=" * 60)
    print("è°ƒè¯•å®Œæˆ")
    print("=" * 60)
    print()
    print("ğŸ’¡ æŸ¥çœ‹ä¸Šé¢çš„æ—¥å¿—ï¼Œå¯ä»¥çœ‹åˆ°ï¼š")
    print("  - HTTP è¯·æ±‚çš„ URL")
    print("  - è¯·æ±‚çš„ JSON å‚æ•°")
    print("  - messages çš„å†…å®¹")
    print("  - å“åº”å†…å®¹")

if __name__ == "__main__":
    debug_with_logging()

